\documentclass[a4paper,twoside]{article}

\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{amsmath,amsthm}
\usepackage{amssymb,amsfonts}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage[obeyspaces]{url}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue,
            citecolor = blue,
            anchorcolor = blue]{hyperref}
\usepackage{color}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{framed}
\usepackage{skull}
\usepackage{stackengine}
\usepackage{scalerel}
\usepackage{mathrsfs}
\usepackage{wasysym}
\usepackage{tikz}
\usepackage{relsize}
\usepackage{cancel}
\usepackage{fontawesome}
\usepackage[font=small]{caption}
\colorlet{shadecolor}{blue!9}
\definecolor{Red}{rgb}{0,0,0.3}

\setlength{\headheight}{14pt}
\setlength{\topmargin}{-1cm}
\setlength{\evensidemargin}{-1cm}
\setlength{\oddsidemargin}{-1cm}
\setlength{\textwidth}{18cm} 
\setlength{\textheight}{25cm}

\linespread{1.2}

\usepackage{lastpage}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhead{} % clear all header fields
\fancyhead[RE]{\slshape \rightmark}
\fancyhead[LO]{\slshape \leftmark}
%\fancyhead[CO,CE]{\bfseries Jacopo D'Aurizio - Esercizi svolti di Analisi 1}
\fancyfoot{} % clear all footer fields
\fancyfoot[CE,CO]{Pagina \thepage\ / \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.2pt}
\renewcommand{\epsilon}{\varepsilon}

%\renewcommand{\thesubsection}{\Roman{subsection}}
\newcommand{\mydef}{\;\dot{=}_{\!\!\!\!\mbox{. }}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\bigo}[1]{O\left(#1\right)}
\newcommand{\smallo}[1]{o\left(#1\right)}
\newcommand{\hard}{(\skull)}
\newcommand{\Conv}[1]{\operatorname{Hull}\left(#1\right)}
\newcommand{\arcsinh}[1]{\operatorname{arcsinh}\left(#1\right)}
\newcommand{\arccosh}[1]{\operatorname{arccosh}\left(#1\right)}
\newcommand{\arctanh}[1]{\operatorname{arctanh}\left(#1\right)}
\newcommand{\emptyline}{$\phantom{}$}
\newcommand{\nline}{$\phantom{}$\\}
\newcommand{\sinc}{\operatorname{sinc}}
\newcommand{\GM}{\operatorname{GM}}
\newcommand{\AM}{\operatorname{AM}}
\renewcommand{\gcd}{\operatorname{mcd}}
\newcommand{\mcd}{\operatorname{mcd}}

\theoremstyle{definition}
\newtheorem{theorem}{\color{Red}\underline{\textrm Teorema}}

\newenvironment{theo}
  {\begin{shaded}\begin{theorem}}
  {\end{theorem}\end{shaded}}

%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{cor}[theorem]{Corollario}
%\newtheorem{definizione}[theorem]{Definizione}
%\newtheorem{esempio}[theorem]{Esempio}
%\newtheorem{ex}[theorem]{Esercizio}

\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{cor}[theorem]{Corollario}
\newtheorem{definizione}[theorem]{Definizione}
\newtheorem{esempio}[theorem]{Esempio}
\newtheorem{ex}[theorem]{Esercizio}
\newtheorem{oss}{\underline{\textrm Osservazione}}
\numberwithin{theorem}{section}

\makeatletter
\DeclareFontFamily{U}{tipa}{}
\DeclareFontShape{U}{tipa}{m}{n}{<->tipa10}{}
\newcommand{\arc@char}{{\usefont{U}{tipa}{m}{n}\symbol{62}}}%

\newcommand{\arc}[1]{\mathpalette\arc@arc{#1}}

\newcommand\dangersign[1][2ex]{%
  \renewcommand\stacktype{L}%
  \scaleto{\stackon[1.3pt]{\color{red}$\triangle$}{\tiny !}}{#1}%
}

\newcommand{\arc@arc}[2]{%
  \sbox0{$\m@th#1#2$}%
  \vbox{
    \hbox{\resizebox{\wd0}{\height}{\arc@char}}
    \nointerlineskip
    \box0
  }%
}
\makeatother
\setcounter{section}{0}
\begin{document}
\setlength\parindent{0pt}
\thispagestyle{empty}
\begin{center}{\large \textbf{ {\Huge Calculus 101 for Jackchans} \\
B.A.M. \& Jack, 2023}}
\end{center}
\rule{\textwidth}{1pt}
\tableofcontents

\section{Sproloquio Introduttivo}
    Queste note hanno lo scopo di essere una raccolta degli esercizi, dei fatti notevoli, dei fatti meno notevoli ma altrettando interessanti appresi, grazie a Jack, durante lo studio per l'esame di Analisi 1 presso l'università di Pisa.
    \\
    Per la natura del materiale presente, queste note non saranno da sole (immagino) sufficienti a preparare l'esame in questione, a causa della possibile brevità di alcune sezioni e spiegazioni. Sono a nostro avviso comunque del buon materiale anche se un $\epsilon$ più tecnico.  
    \\
\section{Definizioni e Richiami}
La serie di Taylor di una funzione $f(x)\in C^{\infty}$, centrata in $x_0$, è definita come:
$$\sum_{n\geq 0}^{}\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n.$$
Se i \emph{coefficienti} $a_n=\frac{f^{(n)}(x_0)}{n!}$ soddisfano definitivamente $|a_n|=O(M^n)$ per qualche $M\in\R^+$, nella palla $|x-x_0|<\frac{1}{M}$ la precedente serie converge puntualmente a $f(x)$, e converge uniformemente su tutti i compatti contenuti nella palla, per confronto con la serie geometrica $\sum_{n\geq 0}M^n z^n = \frac{1}{1-Mz}$. Le serie di Taylor centrate nell'origine sono anche dette serie (o \emph{sviluppi}) di Maclaurin. In termini di quest'ultime, diamo alcune definizioni importanti.
\begin{itemize}
\item La funzione esponenziale può essere definita attraverso $$e^x=\sum_{n\geq 0}\frac{1}{n!}x^n$$ 
e soddisfa l'equazione funzionale $f(x)f(y)=f(x+y)$, nonché l'equazione differenziale $f'(x)=f(x)$\\ con dato iniziale $f(0)=1$.
\item Il seno può essere definito attraverso $$\sin x = \sum_{n\geq 0}\frac{(-1)^{2n+1}}{(2n+1)!}x^{2n+1}$$
ed è soluzione dell'equazione differenziale $f''(x)+f(x)=0$ con dati iniziali $f(0)=0, f'(0)=1$.
\item Il coseno può essere definito attraverso $$\cos x = \sum_{n\geq }\frac{(-1)^n}{(2n)!}x^{2n}$$
ed è soluzione di $f''(x)+f(x)=0$ con dati iniziali $f(0)=1, f'(0)=0$.
\end{itemize}
Le tre serie elencate si estendono impunemente al campo complesso definendo funzioni analitiche e intere (il raggio di convergenza della serie di Taylor, centrata in qualunque punto, è $+\infty$). Con le definizioni adottate è immediato verificare che vale l'\textbf{identità di Eulero-De Moivre} $e^{i\theta}=\cos\theta+i\sin\theta$ per ogni $\theta\in\C$. Restringendosi al caso $\theta\in\R$ si ha che $f(\theta)=e^{i\theta}$ fornisce una parametrizzazione per lunghezza d'arco della circonferenza goniometrica, recuperando così l'usuale definizione ``scolastica'' delle funzioni trigonometriche elementari. Dall'identità di Eulero, considerando la parte reale o la parte immaginaria di $e^{i(\theta+\phi)}=e^{i\theta}\cdot e^{i\phi}$, si ottengono le \textbf{formule di addizione/sottrazione} del seno e del coseno:
$$ \sin(\theta\pm\phi)=\sin\theta\cos\phi \pm \cos\theta\sin\phi $$
$$ \cos(\theta\pm\phi)=\cos\theta\cos\phi \mp \sin\theta\sin\phi $$
e dal caso $\theta=\phi$ si ottengono le \textbf{formule di duplicazione/bisezione}:
$$ \sin(2\theta) = 2\sin\theta\cos\theta $$
$$ \cos(2\theta) = \cos^2\theta-\sin^2\theta = 2\cos^2\theta - 1 = 1-2\sin^2\theta $$
$$ \cos\frac{\theta}{2}=\pm\sqrt{\frac{1+\cos\theta}{2}},\qquad \sin\frac{\theta}{2}=\pm\sqrt{\frac{1-\cos\theta}{2}}.$$
Dalle formule di addizione/sottrazione si ottengono altresì le \textbf{formule di prostaferesi}, che permettono di convertire somme di seni o coseni in prodotti di seni o coseni e viceversa:
$$ 2\sin\theta\sin\phi = \cos(\theta-\phi)-\cos(\theta+\phi)\qquad \cos\alpha-\cos\beta = 2\sin\frac{\alpha+\beta}{2}\sin\frac{\beta-\alpha}{2} $$
$$ 2\sin\theta\cos\phi = \sin(\theta+\phi)+\sin(\theta-\phi)\qquad \sin\alpha+\sin\beta = 2\sin\frac{\alpha+\beta}{2}\cos\frac{\alpha-\beta}{2} $$
$$ 2\cos\theta\cos\phi = \cos(\theta+\phi)+\cos(\theta-\phi),\qquad \cos\alpha+\cos\beta = 2\cos\frac{\alpha+\beta}{2}\cos\frac{\alpha-\beta}{2}. $$

\begin{ex} Si dimostri che le somme parziali della successione $\{\sin n\}_{n\geq 1}$ sono limitate e si determini esplicitamente il valore del loro estremo superiore. 
\end{ex}
\textbf{Soluzione}. Posto $A_N=\sum_{n=1}^{N}\sin n$, per le formule di prostaferesi si ha 
$$ 2\sin\tfrac{1}{2}\sum_{n=1}^{N}\sin n = \sum_{n=1}^{N}\left(\cos\left(n-\tfrac{1}{2}\right)-\cos\left(n+\tfrac{1}{2}\right)\right)=\cos\left(\tfrac{1}{2}\right)-\cos\left(N+\tfrac{1}{2}\right) ,$$
$$ A_N = \frac{\cos\left(\tfrac{1}{2}\right)-\cos\left(N+\tfrac{1}{2}\right)}{2\sin\tfrac{1}{2}} $$
e poiché la successione $\{\cos\left(N+\frac{1}{2}\right)\}_{N\geq 0}$ è densa in $[-1,1]$, dalle formule di duplicazione si ha 
$$ \sup_N A_N = \frac{1+\cos\tfrac{1}{2}}{2\sin\tfrac{1}{2}} = \frac{2\cos^2\tfrac{1}{4}}{4\sin\tfrac{1}{4}\cos\tfrac{1}{4}} = {\color{blue}\tfrac{1}{2}\cot\tfrac{1}{4}}<2.$$
\\

\section{Limiti e Continuità}
\subsection{Limiti di Successioni}
Saranno date per buone le definizioni di Spazio Metrico e Topologico.

\begin{definizione}
Una successione $\{a_n\}_{n\in\N}$ si dice: \\
\textit{inferiormente limitata} se  esiste $m$ tale che $\forall n, a_n\geq m$ \\
\textit{superiormente limitata} se  esiste $m$ tale che $\forall n, a_n\leq M$ \\
\textit{limitata} se eistono due numeri $m$ ed $M$ tali che $\forall n, m\leq a_n\leq M$
\end{definizione}

\begin{definizione}
Una successione $\{a_n\}$ si dice convergente se $\exists l\in E$ tale che: $$\forall \epsilon>0,\text{ definitivamente } |a_n-l|<\epsilon$$ \\
$l$ si chiama \textbf{limite della successione $a_n$}:
$$\lim_{n\to+\infty}a_n=l.$$
\end{definizione}

\textbf{Nota}: l'operatore $\lim$ è \textbf{lineare}.

\textbf{Definizione}
Una successione $\{a_n\}$ si dice \emph{crescente} se $$\forall n, a_{n+1} > a_n$$
 e si dice \emph{debolmente crescente} se vale la disuguaglianza $a_n\leq a_{n+1}$. È definito analogamente il concetto di decrescenza. Sia le successioni crescenti che quelle decrescenti sono dette monotone.\\
 
 
\begin{theorem}[Monotonia]
Sia $\{a_n\}$ una successione monotona crescente e superiormente limitata.\\ Allora $a_n$ converge e il suo limite è $\sup_{n\in\N} a_n$. Vale l'analogo con l'$\inf$ per le successioni decrescenti.
\end{theorem}
\begin{proof} Senza perdità di generalità dimostriamo il caso delle successioni monotone crescenti e limitate. \\
Poiché $\{a_n\}$ è limitata esiste finito $\Gamma=\sup_{n\in\N}a_n$ e questo realizza $\Gamma \geq a_n$ per ogni $n\in\N$.\\
Per proprietà del $\sup$, in ogni intorno sinistro di $\Gamma$ vi è almeno un elemento della successione, in particolare
$$ \forall \epsilon > 0, \exists n\in \N: \Gamma-\epsilon < a_n \leq \Gamma. $$
Poiché la successione è crescente e limitata da $\Gamma$ ne consegue
$$ \forall \epsilon > 0, \exists n\in \N: \forall m\geq n, \Gamma-\epsilon < a_m \leq \Gamma, $$
e questo dimostra che 
$$\lim_{n\to\infty}a_n = \Gamma.$$
\end{proof}


\begin{theorem}[Permanenza del segno]
Se $a_n \to a$ e $a>0$ allora $a_n>0$ definitivamente.
(vale anche col minore di 0)
\end{theorem}
\begin{proof}
Vale che $$\forall\epsilon>0,\text{ definitivamente }|a_n-a|<\epsilon$$
che è equivalente a:
$$a-\epsilon<a_n<a+\epsilon$$
ma allora per ogni $\epsilon < a$ si ha $a_n>0$.
\end{proof}

\begin{theorem}[Del confronto o \emph{dei due carabinieri}]
se $a_n\leq b_n \leq c_n$ definitamente e: 
$$a_n\to l, c_n \to l \in \R$$
allora anche $b_n\to l$.
\end{theorem}
\begin{proof}
Per definizione di limite, si ha che, comunque fissato $\epsilon >0$, definitivamente sia $\{a_n\}$ che $\{c_n\}$ giacciono in un $\epsilon$-intorno di $l$. Da $a_n\leq b_n\leq c_n$ segue che anche $\{b_n\}$ deve  definitivamente appartenere ad un $\epsilon$-intorno di $l$, e vista l'arbitrarietà di $\epsilon$ si ha $\lim_{n\to +\infty} b_n = l$. 
\end{proof}

\begin{definizione} Una \emph{sottosuccessione} di $\{a_n\}$ è una qualunque successione della forma $\{a_{n_k}\}_{k\geq 1}$\\ con $n_1 < n_2 < n_3 < \ldots$. 
\end{definizione}

\begin{theorem}[Bolzano-Weierstrass]
Ogni successione di numeri reali contenuta in un intervallo limitato $[a,b]$ possiede una sottosuccessione convergente in $[a,b]$.\end{theorem}
\begin{proof}
Sia $\{x_n\}$ la nostra successione, Dividiamo l'intervallo $[a,b]$ in due sotto intervalli di uguale ampiezza. Almeno uno di questi intervalli conterrà infiniti elementi di $\{x_n\}$, scegliamone uno, $x_0$. Chiamiamo questo intervallo $[a_1,b_1]$, peschiamo un elemento da $[a_1,b_1]$, $x_1$ e procediamo così ottenendo una successione di intervalli $[a_k,b_k]$ e una sottosuccessione $x_{n_k}$ con le seguenti proprietà:
\begin{itemize}
\item $\{a_k\}$ è monotona crescente e superiormente limitata da $b$
\item $\{b_k\}$ è monotona decrescente e inferiormente limitata da $a$
\item $b_k-a_k=\frac{b-a}{2^k}$
\item $x_{n_k}\in [a_k,b_k]$
\end{itemize}
le prime due convergono per il teorema di monotonia e tendono allo stesso limite, $c$.\\ Per confronto anche $x_{n_k}\to c\in [a,b]$.
\end{proof}

\begin{definizione}[Successione di Cauchy]
Si dice che la successione $\{a_n\}$ è \emph{di Cauchy} se
$$\forall\epsilon>0 \;\exists\, n_0:\forall n,m\geq n_0, |a_n-a_m|<\epsilon.$$
\end{definizione}
Vista la somiglianza con la definizione di successione convergente, è semplice verificare che\\
$(i)$ se una successione è convergente allora è di Cauchy \\
$(ii)$ se una successione è di Cauchy allora è limitata.\\
Vale inoltre il seguente
\begin{theorem}[Completezza dei reali]
Se $\{a_n\}$ è una successione di Cauchy di reali, $a_n\to l\in\mathbb{R}$.\end{theorem}
\begin{proof}
Poiché $\{a_n\}$ è di Cauchy questa è limitata. Per Bolzano-Weierstrass ammette allora una sottosuccessione $\{a_{n_k}\}$ convergente ad un certo limite $l$. 
Mostriamo che l'intera successione converge ad $l$. Infatti:
$$|a_n-l|\leq|a_n-a_{n_k}| +|a_{n_k}-l|$$ e fissato $\epsilon>0, |a_{n_k}-l|<\epsilon$ definitivamente e quindi in particolare vale $|a_n-a_{n_k}|<\epsilon$ definitivamente, perciò: $$|a_n-l|<2\epsilon$$
definitivamente.
\end{proof}

\textbf{Nota}: l'enunciato in $\Q$ è falso. Basta considerare la successione a valori in $\Q$ data da $\left\{\frac{F_{n+1}}{F_n}\right\}_{n\geq 0}$, dove il termine generale è il rapporto tra due numeri di Fibonacci consecutivi. Questa è una successione di Cauchy sia in $\Q$ che in $\R$, ma il suo limite (ossia il rapporto aureo) non appartiene a $\Q$.

\subsection{Continuità}
\begin{definizione}[Continuità]
Se $f:I\to\R$, $I$ un intervallo, e $c\in I$ si dice che $f$ è \emph{continua in $c$} se esiste $\lim_{x\to c}f(x)$ e questo coincide con $f(c)$. 
$f$ si dice \emph{continua} in $I$ se è continua in ogni punto di $I$. \end{definizione}

\begin{theorem}[Degli zeri] Sia $f$ continua in $[a,b]$ e $f(a)\cdot f(b)<0$ allora esiste $c\in (a,b)$ tale che $f(c)=0$.\\ Se $f$ è strettamente monotona lo zero è unico per iniettività di $f$.
\end{theorem}
\begin{proof}
Costruiamo una successione che tende ad uno zero di $f$. Posto $c_1=\frac{a+b}{2}$ se $f(c_1)=0$ abbiamo vinto, altrimenti guardiamo il segno di $f(a)\cdot f(c_1)$. Ora se $f(a)\cdot f(c_1)<0$ consideriamo $[a_1,b_1]$ con $a_1=a$ e $b_1=c_1$, altrimenti $a=c_1$ e $b_1=b$. ponendo adesso $c_2=\frac{a_1+b_1}{2}$ possiamo ripetere la procedura di sopra fino a che non troviamo lo zero cercato. Infatti le successioni $\{a_n\}$ e $\{b_n\}$ sono limitate e monotone, hanno quindi limiti finiti, che chiamiamo rispettivamente $l_1$ ed $l_2$. 
Osservando che $a_n - b_n\to 0$ abbiamo che $l_1=l_2=l$, dunque $f(l)=0$.
\end{proof}


\begin{theorem}[Weierstrass]
Se $f:[a,b]\to \R$ una funzione continua, allora $f$ assume massimo e minimo in $[a,b]$.\end{theorem}
\begin{proof} È sufficiente provare che per qualche $c\in[a,b]$ si ha $f(c)=\sup_{x\in[a,b]}f(x)$, per poi applicare il medesimo lemma a $-f(x)$. Proviamo preliminarmente che $f$ è necessariamente superiormente limitata. Se così non fosse, esisterebbe una qualche successione $\{x_n\}_{n\geq 1}$ di elementi di $[a,b]$ tale per cui $f(x_n)\geq n$ per ogni $n\in\N^+$.\\ Per Bolzano-Weierstrass esisterebbe una estratta $\{x_{n_k}\}_{k\geq 1}$ convergente a $\overline{x}\in[a,b]$, e dalla continuità di $f$ si avrebbe 
$$f(\overline{x})=\lim_{k\to +\infty} f(x_{n_k}) = +\infty, $$
contro l'ipotesi che $f$ abbia valori reali in tutti i punti di $[a,b]$. Provato che l'immagine di $[a,b]$ secondo $f$ è superiormente limitata, poniamo $\Gamma=\sup_{x\in[a,b]}f(x)$. Per definizione di $\sup$, per ogni $n\in\N^+$ esiste un $x_n\in[a,b]$ tale per cui $f(x_n)\geq \Gamma-\frac{1}{n}$. Analogamente a prima, dalla successione $\{x_n\}_{n\geq 1}$ è possibile estrarre una sottosuccessione $\{x_{n_k}\}_{k\geq 1}$ convergente a $\overline{x}\in[a,b]$, e dalla continuità di $f$ segue 
$$ f(\overline{x})= \lim_{k\to +\infty} f(x_{n_k}) = \Gamma. $$
 
\end{proof}


\begin{theorem}[Dei valori intermedi]
Se $f:[a,b]\to \R$ è una funzione continua, allora $f$ sul suo dominio assume tutti i valori compresi tra $f(a)$ e $f(b)$.
\end{theorem}
\begin{proof}
Se $f(a)=f(b)$ non c'è alcunché da dimostrare. Possiamo allora assumere $f(a)\neq f(b)$ e per qualunque $c$ compreso tra $f(a)$ ed $f(b)$ applicare il teorema degli zeri a $f(x)-c$. 
\end{proof}


\begin{theorem}[Invertibilità]
Una funzione continua $f$ da un intervallo chiuso $I$ in $\R$ è invertibile se e solo se è strettamente monotona, e in tal caso ha inversa continua e strettamente monotona. 
\end{theorem}
\begin{proof}

Se $f:I\to \R$ continua e invertibile non fosse strettamente monotona, esisterebbero tre punti ${x_1<x_2<x_3}$ in $I$ tali che $f(x_2)\leq \min(f(x_1),f(x_3))$ oppure $f(x_2)\geq \max(f(x_1),f(x_3))$. In entrambi i casi, dal Teorema dei valori intermedi seguirebbe $f(\alpha)=f(\beta)$ con $\alpha\in[x_1,x_2]$, $\beta\in[x_2,x_3]$ e $\alpha\neq\beta$, contro l'iniettività di $f$.\\
Per quanto concerne la seconda parte, consideriamo una generica $f:I\to\mathbb{R}$ continua e strettamente monotona. Per Bolzano-Weierstrass questa manda i punti dell'intervallo $I$ nei punti di un intervallo $J$, e a meno di ``rovesciare'' gli elementi del dominio possiamo supporre che $f$ mandi con continuità $I$ in $J$ preservando l'ordine stretto. Questo ci dà immediatamente la stretta monotonia della funzione inversa $g:J\to I$, e resta da provare unicamente la continuità di $g$. Supponiamo che $\{y_n\}_{n\geq 1}$ sia una successione di elementi di $J$ convergente a $\overline{y}\in J$, e poniamo $x_n=g(y_n)$, equivalente a $y_n=f(x_n)$. Per Bolzano-Weierstrass $\{x_n\}_{n\geq 1}$ ammette una estratta $\{x_{n_k}\}_{k\geq 1}$ che è sia strettamente monotona che convergente a $\overline{x}\in I$. Per continuità di $f$ la successione $\{y_{n_k}\}_{k\geq 1}$ converge a $f(\overline{x})$. Poiché $\{y_n\}_{n\geq 1}$ è convergente per ipotesi, qualunque sua estratta deve ammettere lo stesso limite, $\overline{y}=f(\overline{x})$. Segue che l'intera successione $\{x_n\}_{n\geq 1}$ converge a $\overline{x}$ e che $g$ manda successioni convergenti in successioni convergenti, ossia è (sequenzialmente) continua. Se infatti $\{x_n\}_{n\geq 1}$ ammettesse estratte convergenti a punti diversi, per continuità e stretta monotonia di $f$ lo stesso varrebbe per $\{y_n\}_{n\geq 1}$, contro le ipotesi.
\end{proof}


\begin{definizione}[Continuità uniforme]
Si dice che $f:I\to\R$ è uniformemente continua in $I$ se 


$$\forall\epsilon>0\; \exists\,\delta>0: \forall x_1,x_2\in I\text{ vale che } |x_1-x_2|<\delta \Rightarrow |f(x_1)-f(x_2)|<\epsilon.$$
\end{definizione}

\begin{theorem}[Heine-Cantor]
Se $f:[a,b]\to\R$ è continua, allora è uniformemente continua in $[a,b]$.
\end{theorem}
\begin{proof}
Comunque fissato $\epsilon > 0$, per ogni $x\in[a,b]$ esiste un $\delta_x > 0$ che assicura $$|z-x|<\delta_x\Longrightarrow|f(z)-f(x)|<\epsilon.$$
Gli intervalli della forma $(x-\delta_x, x+\delta_x)$ costituiscono un ricoprimento aperto di $[a,b]$. Per compattezza di $[a,b]$ esiste un sotto-ricoprimento finito costituito da $(x_1-\delta_{x_1},x_1+\delta_{x_1}),\ldots,(x_n-\delta_{x_n},x_n+\delta_{x_n})$. Posto $\delta=\max(\delta_{x_1},\ldots,\delta_{x_n})$ si ha di conseguenza che sul dominio di $f$
$$|z-x|<\delta\Longrightarrow|f(z)-f(x)|<\epsilon.$$
\end{proof}


\begin{definizione}[Modulo di continuità]
Sia $f:(a,b)\to\R$ una funzione a valori in $\R$, e sia $\delta$ un numero reale positivo. Si definisce \emph{modulo di continuità} locale di $f$ in $x_0$ una funzione $\omega_0:\R^+\to\R^+$  tale che:\\
$$|f(x_0)-f(x)|\leq\omega_{x_0}(\delta),\forall x\in(a,b):|x_0-x|\leq\delta$$ è detto modulo di continuità globale invece:
$$|f(x_0)-f(x)|\leq\omega_{x_0}(\delta),\forall x,x_0\in(a,b):|x_0-x|\leq\delta$$
\end{definizione}



Il modulo di continuità misura l'uniforme continuità di $f$, in particolare valgono le seguenti proprietà:
\begin{itemize}
 \item $f$ è continua in $x_0$ se e solo se essa ammette un modulo di continuità locale $\omega_{x_0}$ tale che $\lim_{\delta\to0}\omega_{x_0}(\delta)=0$
 \item una funzione è uniformemente continua se e solo se ammette un modulo di continuità globale $\omega_f$ tale che $\lim_{\delta\to 0}\omega_f(\delta)=0$
 \item per funzioni derivabili su un intervallo e lipschitziane di costante $L$ il modulo di continuità ha crescita sub-lineare, cioè $\omega_f(\delta)\leq C\delta$.
\end{itemize}


\begin{theorem}[Integrabilità delle funzioni continue]
Se $f:[a,b]\to\R$ è continua, allora è Riemann-integrabile su $[a,b]$.
\end{theorem}
\begin{proof}
Data una partizione di $[a,b]$ di calibro $\delta$, l'uniforme continuità di $f$ assicura che su ogni componente della partizione si abbia $\sup f-\inf f \leq \epsilon$. In particolare la differenza tra l'$\inf$ delle somme di Riemann superiori e il $\sup$ delle somme di Riemann superiori è controllato da $\epsilon(b-a)$. Dall'arbitrarietà di $\epsilon$ segue la Riemann-integrabilità di $f$. 
\end{proof}

Per quanto la dimostrazione richieda elementi di Teoria della Misura, è importante sapere che vale anche una sorta di viceversa:

\begin{theorem}[Riemann-Lebesgue] Se $f:[a,b]\to\mathbb{R}$ è Riemann-integrabile, l'insieme dei punti di discontinuità di $f$ ha misura di Lebesgue nulla. 
\end{theorem}



\subsection{Spazi metrici e di Banach}
Sia $X$ un insieme e sia $d:X \times X\to [0,+\infty]$ una funzione che ad ogni coppia $(x,y)$ di punti in $X$ associa un numero reale $d(x,y)\geq0$. Si dice che $d$ è una distanza o metrica su $X$ se sono verificate le seguenti:
\begin{itemize}
\item  $d(x,y)=0 \Leftrightarrow x=y\quad\forall x,y\in X$
\item $d(x,y)=d(y,x)\quad\forall x,y\in X$
\item $d(x,y)\leq d(x,z)+d(z,y)\quad\forall x,y,z\in X$
\end{itemize}
L'ultima condizione prende il nome di \textbf{disuguaglianza triangolare}.\\
La coppia $(X,d)$ si dice \textbf{spazio metrico} se $d$ è una distanza (o metrica).

\begin{definizione}[Intorno circolare]
Per ogni $x_0\in X$ e per ogni $r>0$, si chiama Intorno Circolare (Intorno Sferico o Palla Aperta) di centro $x_0$ e di raggio $r$ l'insieme $$B_r(x_0)=\{x\in X: d(x,x_0)<r\}.$$
\end{definizione}

\begin{definizione}[Aperti e Topologia]
Un insieme $A\subseteq X$ Si dice aperto se ogni suo punto è centro di una palla aperta contenuta in $A$, cioè se per ogni suo punto $x_0\in A$ esiste $r>0$ tale che $B_r(x_0)\subseteq A$ (si assume il vuoto come un insieme aperto). Un insieme $C\subseteq X$ si dice chiuso se il suo complementare, $A=X\setminus C$ è aperto. L'insieme di tutti gli aperti di uno spazio metrico $(X,d)$ si dice \textbf{topologia generata dalla metrica $d$}.
\end{definizione}

\textbf{Proposizione}. In uno spazio metrico, tutti le palle aperte, le unioni arbitrarie di aperti e le intersezioni finite di aperti sono aperte.\\
\textbf{Proposizione}. In uno spazio metrico intersezioni arbitrarie e unioni finite di chiusi danno luogo a insiemi chiusi.\\

Uno stesso insieme può avere metriche diverse a seconda della funzione $d$. Ad esempio: \\
Sia $(X,d)$ una metrica dove $X$ è un insieme e

$$ d(x,y)=\left\{\begin{array}{ccl} 0 &\text{se} & x=y\\ 1 &\text{se} & x\neq y\end{array}\right. $$

è chiaro che $d$ è una metrica, che prende il nome di metrica discreta. Segue anche che, secondo questa metrica, ogni sottoinsieme di $X$ è aperto.\\
Invece $d(x,y)=|x-y|$ definisce una metrica che prende il nome di euclidea, la quale genera l'usuale topologia della retta reale. In questo caso un insieme è aperto se e solo se è intorno di ogni suo punto.\\
Si può definire la convergenza di successioni e di funzioni continue attraverso una metrica. Infatti sia $a_n$ una successione in $X$, si dice che la successione converge, o tende, a $x_0\in X$ se per ogni $\epsilon>0$ esiste $v\in \N$ tale che, per ogni $k>v$: \\
$$d(a_k,x_0)<\epsilon.$$

Si prova in maniera analoga a come si fa in $\R$ che vale il \textbf{Teorema di unicità del limite}.
\begin{theorem}
Sia $(X,d)$ uno spazio metrico e sia $x_0$ un fissato punto di $X$. Allora la funzione: $$x\in X\to d(x,x_0)$$
è continua da $X$ verso $\R$ (dotato della metrica euclidea)
\end{theorem}
\begin{proof}
    Applicando la disuguaglianza di Lipschitzianità della distanza si ha che per $x,y\in A$:
    $$|d(y,x_0)-d(x,x_0)|\leq d(x,y)$$
    Allora se $x_0$ è una successione di punti convergenti ad $x$, si ha: 
    $$|d(x_k,x_0)-d(x,x_0)|\leq d(x_k,x)$$
    e perciò $d(x_k,x_0)\to d(x,x_0)\text{ per } k\to\infty$.
\end{proof}
\begin{definizione}[Spazio normato]
Sia $V$ uno spazio vettoriale. Una norma su $V$ è una funzione che ad ogni vettore $x\in V$ associa un reale $\|x\|\geq 0$ che verifica le seguenti condizioni:
\begin{itemize}
    \item $\|x\|=0 \text{ se e solo se } x=0 \qquad \forall x\in V$
    \item $\|\lambda x\|=|\lambda|\cdot\|x\|\qquad \forall\lambda\in\R,\; \forall x\in V$
    \item $\|x+y\|\leq \|x\|+\|y\|\qquad \forall x,y\in V$
\end{itemize}
\end{definizione}
L'ultima disequazione prende il nome di \emph{disuguaglianza triangolare}.
\begin{definizione}[Spazio di Banach]
Sia $(X,d)$ uno spazio normato, se esso risulta completo come spazio metrico rispetto alla distanza: $$d(x,y)=\|x-y\|_V$$ generata dalla norma $\|\cdot\|_V$, diremo che $V$ è uno spazio di Banach.    
\end{definizione}
\subsection{Funzioni Lipschitziane e teorema delle contrazioni}
\begin{definizione}[Funzione L-Lip]
Siano $(X,d_X)$ e $(Y,d_Y)$ due spazi metrici e sia $f:X\to Y$ una funzione.\\ Diremo che $f$ è lipschitziana se esiste una costante $L$ tale che: $$d_Y(f(x),f(y))\leq L\cdot d_X(x,y)\qquad \forall x,y\in X$$    
\end{definizione}
Se $f:X\to Y$ è L-Lip chiaramente è anche continua in $X$.
\begin{definizione}[Contrazione]
Una $f:X\to X$ Lipschitziana con costante $L<1$, cioè tale per cui
$$\exists L\in[0,1): d(f(x),f(y))\leq L\cdot d(x,y)\qquad\forall x,y\in X$$
prende il nome di \emph{contrazione} sullo spazio $(X,d)$.
\end{definizione}
\begin{theorem}[Teorema delle Contrazioni (Banach)]
Sia $(X,d)$ uno spazio metrico completo e sia $f:X\to X$\\ una contrazione. Allora esiste uno ed un solo punto $x\in X$ tale che $f(x)=x$. $x$ si chiama \emph{punto fisso}.    
\end{theorem}
\begin{proof} A partire da un qualunque $x_0\in X$ possiamo definire una successione a valori in $X$\\ attraverso $x_{n+1}=f(x_n)$. Si ha che 
$$d(x_{n+2},x_{n+1}) = d(f(x_{n+1}),f(x_n)) \leq L\cdot d(x_{n+1},x_n), $$
$$d(x_{n+3},x_{n+2}) \leq L\cdot d(x_{n+2},x_{n+1}) \leq L^2\cdot d(x_{n+1},x_n)$$
e per induzione $d(x_{n+k+1},x_{n+k})\leq L^k\cdot d(x_{n+1},x_{n})$. Dalla disuguaglianza triangolare segue pertanto 
$$ d(x_{n+k+1},x_{n}) \leq \left(1+L+L^2+\ldots+L^k\right)\cdot d(x_{n+1},x_n)\leq \frac{d(x_{n+1},x_n)}{1-L}\leq \frac{L^n}{1-L}\cdot  d(x_1,x_0).$$
L'ultima disuguaglianza comporta che $\{x_n\}_{n\geq 0}$ sia una successione di Cauchy e la completezza di $X$ comporta che $x_n\to \overline{x} \in X$. Dalla continuità di $f$ segue inoltre $x_{n+1}=f(x_n)\to f(\overline{x})$, dunque $\overline{x}$ è necessariamente un punto fisso di $f$. Abbiamo infine che $\overline{x}$ non dipende da $x_0$: se $f$ avesse due distinti punti fissi $\overline{x}_1$ e $\overline{x}_2$ avremmo 
$$ d(\overline{x}_1,\overline{x}_2) = d(f(\overline{x}_1),f(\overline{x}_2)) \leq L\cdot d(\overline{x}_1,\overline{x}_2) < d(\overline{x}_1,\overline{x}_2) $$
che è una contraddizione. Segue che per qualunque $x_0\in X$ la successione data da $x_{n+1}=f(x_n)$ converge verso l'unico punto fisso di $f$.   
\end{proof}

La stessi tesi vale in ipotesi leggermente meno forti.

\begin{definizione}[Funzione contrattiva o contrazione debole] Se $(X,d)$ è uno spazio metrico e $f:X\to X$ è tale per cui  $d(f(x),f(y))<d(x,y)$ per ogni $x,y$ distinti in $X$, $f$ è detta \emph{funzione contrattiva} o \emph{contrazione debole}. 
\end{definizione}

\begin{theorem}[Teorema delle contrazioni deboli (Banach-Caccioppoli)] Se $(X,d)$ è uno spazio metrico compatto rispetto alla topologia indotta da $d$) e $f:X\to X$ è una contrazione debole, per ogni $x_0\in X$ la successione definita da $x_{n+1}=f(x_n)$ converge all'unico punto fisso di $f$. 
\end{theorem}

\begin{proof}
In quanto funzione $1$-Lip, $f$ è uniformemente continua e lo stesso vale per $g(x)=d(f(x),x)$.\\ Per il Teorema di Weierstrass una funzione continua su un compatto ammette minimo, e il minimo di $g(x)$ su $X$ deve essere necessariamente zero. Se infatti il valore minimo di $g$ fosse $m>0$, assunto in $\overline{x}$, per contrattività di $f$ si avrebbe 
$$ g(f(\overline{x})) = d(f(f(\overline{x})),f(\overline{x})) < d(f(\overline{x}),\overline{x}) = m $$
contro la minimalità di $m$. Questo prova che $f$ ha necessariamente un punto fisso in $X$, e per discorsi analoghi a quelli appena fatti, il punto fisso deve essere unico. Per Bolzano-Weierstrass qualunque successione $\{x_n\}_{n\geq 0}$ ammette una sottosuccessione $\{x_{n_k}\}_{k\geq 0}$ convergente, e per continuità di $f$ tale sottosuccessione è necessariamente convergente a $\overline{x}$. La contrattività di $f$ comporta ora che l'intera successione $\{x_n\}_{n\geq 0}$ sia convergente a $\overline{x}$. Per ogni $\epsilon > 0$,  dalla convergenza di $\{x_{n_k}\}_{k\geq 0}$ abbiamo che definitivamente, diciamo per $k\geq K$, $x_{n_k}$ è in un intorno di raggio $\epsilon$ di $\overline{x}$.\\ 
Per contrattività di $f$ la successione $\{g(x_n)\}_{n\geq 0}$ è strettamente decrescente, dunque converge al suo $\inf$.\\ Poiché $g(x_{n_k})\to g(\overline{x}) = 0$, il precedente $\inf$ è nullo e $d(x_n,f(x_n))$ converge a zero.\\ Per unicità del punto fisso di $f$, $x_n\to\overline{x}$.\end{proof}

\subsection{Compattezza}
\begin{definizione}[Insieme compatto]
    Un sottoinsime $K$ dello spazio metrico $(X,d)$ si dice (sequenzialmente) compatto se da ogni successione $\{x_k\}$ di punti di $K$ si può estrarre una sottosuccessione convergente verso un punto $x\in K$.
\end{definizione}
Osservimo che, se $K\subseteq X$ è un insieme compatto, si dice, secondo questa definizione, che è \emph{compatto per successioni}, a dispetto della definizione di compattezza data attraverso dei \emph{ricoprimenti aperti}. In ogni caso, le due definizioni risultano equivalenti.
\begin{theorem}
    Se $(X,d)$ è uno spazio metrico e $K\subseteq X$ è un insime compatto, allora $K$ è chiuso.
\end{theorem}
\begin{proof}
    sia $x_k$ una successione di punti di $K$ convergente verso il punto $x\in X$. Dimostriamo che $x\in K$. Essendo $K$ compatto estraiamo da $x_k$ una sottosuccessione $x_{k_h}$, questa è convergente verso il punto $x_0\in K$. Necessariamente $x=x_0$ e si ha che $x\in K$
\end{proof}
\begin{theorem}[Heine-Borel]
Un sottoinsieme $K$ di $\R^n$ è compatto se e solo se è chiuso e limitato.    
\end{theorem}
\begin{proof}
 Se $K$ è chiuso e limitato, sia $\{x_k\}_{k\geq 1}=\{\left(x_{k_1},x_{k_2},\dots,x_{k_n}\right)\}_{k\geq 1}$ una successione di punti di $K$.\\ Le successioni $\{x_{k_i}\}_{k\geq 1}$ sono successioni limitate di reali. Per il teorema di Bolzano-Weierstrass dalla successione $\{x_k\}_{k\geq 1}$ se ne può estrarre una avente la prima coordinata convergente; da questa un'altra con seconda coordinata convergente, e così via fino ad $n$. Si ottiene così una successione strettamente crescente $\{k_h\}_{h\geq 1}$ di numeri naturali tale che  $x_{k_h,i}\to x_i$ per ogni $i=1,2,3,\dots,n$. Pertanto $x_k\to x=(x_1,x_2,\dots,x_n)$.\\
 Essendo $K$ chiuso ne segue che $x\in K$ e quindi $K$ è compatto.\\ Viceversa se $K$ è compatto esso è chiuso per la proposizione precedente. Se $K$ non fosse limitato, esisterebbe una successione $\{x_k\}_{k\geq 1}$ di punti di $K$ tale che: $$\lim_{k\to\infty}|x_k|=+\infty$$ e da questa non si potrebbe estrarre alcuna successione convergente, contro l'ipotesi che $K$ sia compatto.
\end{proof}
\subsection{Calcolo differenziale}
Saranno elencati i principali risultati sulle funzioni continue e derivabili in intervalli, dei must have in ogni corso di Analisi 1.
\begin{theorem}[Rolle]
Sia $f:[a,b]\to\R$ una funzione continua e derivabile in $(a,b)$ e tale che $f(a)=f(b)$.\\ Allora esiste un punto $c\in(a,b)$ tale che $f'(c)=0$.   
\end{theorem}
\begin{proof}
    Per il teorema di Weierstrass f ammette massimo e minimo in $[a,b]$, rispettivamente $x_M$ e $x_m$. Abbiamo quindi 2 casi:
    il minimo e il massimo sono prorprio gli estremi dell'intervallo $[a,b]$, allora il massimo di $f$ coincide col suo minimo, quindi la funzione vale costantemente $f(x_M)=f(x_m)=k$, cioè ha derivata costantemente nulla. \\
    Alternativamente almeno uno tra $x_m$ e $x_M$ cade in $(a,b)$, quindi la derivata si annulla in almeno in quel punto.
\end{proof}
\begin{theorem}[Lagrange o Valor medio]
Sia $f:[a,b]\to\R$ e derivabile in $(a,b)$. Esiste un punto $\xi\in(a,b)$ tale che $$f(b)-f(a)=f'(\xi)(b-a).$$
\end{theorem}
\begin{proof}
    La funzione $$g(x)=f(x)-f(a)-\frac{f(b)-f(a)}{b-a}(x-a)$$ è continua in $[a,b]$ e derivabile in $(a,b)$. Inoltre si ha $g(a)=g(b)=0$, quindi per il teorema di Rolle esiste un punto $\xi\in(a,b)$ tale che $g'(\xi)=0$ da cui: $$f'(\xi)=\frac{f(b)-f(a)}{b-a}$$
\end{proof}
\begin{oss}
    Il teorema di Lagrange ha un'interessante interpretazione geometrica: la retta che unisce i punti $(a,f(a))$ e $(b,f(b))$ ha equazione $y=f(a)+\frac{f(b)-f(a)}{b-a}(x-a)$, dunque di coefficiente angolare $\frac{f(b)-f(a)}{b-a}$, mentre $f'(\xi)$ è il coefficiente angolare della retta tangente ad $f$ nel punto $\xi$. Il teorema di Lagrange assicura che esiste un punto dove la tangente al grafico della funzione ha lo stesso coefficiente angolare della retta che unisce gli estremi del grafico.
\end{oss}
\begin{theorem}[Cauchy]
    Siano $f(x)$ e $g(x)$ continue in $[a,b]$ e derivabili in $(a,b)$. Esiste un punto $\xi\in(a,b)$ in cui
    $$[g(b)-g(a)]f'(\xi)=[f(b)-f(a)]g'(\xi).$$
\end{theorem}
\begin{proof}
    La funzione: $$h(x)=[g(b)-g(a)]f(x)-[f(b)-f(a)]g(x)$$ verifica tutte le ipotesi del teorema di Rolle, allora esiste un punto $\xi\in(a,b)$ in cui questa si annulla. 
\end{proof}
\begin{theorem}[De l'H\^opital 1]
    Siano $f(x)$ e $g(x)$ continue in $[a,b]$ e derivabili in $(a,b)$, con la possibile eccezione di un punto $x_0$. Supponiamo $f(x_0)=g(x_0)=0$ e che $g(x)$ e $g'(x)$ non si annullino mai per  $x\neq x_0$. Supponiamo anche che esista finito il limite del rapporto delle derivate: $$\lim_{x\to x_0}\frac{f'(x)}{g'(x)}=L.$$ Allora esiste anche il rapporto delle funzioni ed è uguale al precedente: $$\lim_{x\to x_0}\frac{f(x)}{g(x)}=L.$$ 
\end{theorem}
\begin{proof}
    Sia $x\in(a,b)$, per Cauchy, esiste un punto $\xi(x)\in(x_0,x)$ (anche se dipende da $x$ lo chiameremo semplicemente $\xi$ per non appesantire la notazione) tale che $$\frac{f(x)}{g(x)}=\frac{f(x)-f(x_0)}{g(x)-g(x_0)}=\frac{f'(\xi)}{g'(\xi)}$$ quando $x\to x_0$ anche $\xi\to x_0$, si ha pertanto $$\lim_{x\to x_0}\frac{f(x)}{g(x)}=\lim_{x\to x_0}\frac{f'(\xi)}{g'(\xi)}$$
\end{proof}
\begin{theorem}[De l'H\^opital 2]
    Siano $f(x)$ e $g(x)$ due funzioni derivabili in $[a,b]\setminus\{x_0\}$. Supponiamo che per $x\to x_0$ entrambe tendano a $+\infty$ e che $g'(x)$ non si annulli mai in un intorno di $x_0$. Supponiamo infine che esista finito il rapporto delle derivate $$\lim_{x\to x_0}\frac{f'(x)}{g'(x)}=L.$$
    Allora esiste anche il rapporto delle funzioni ed è uguale al precedente: $$\lim_{x\to x_0}\frac{f(x)}{g(x)}=L.$$
\end{theorem}
\begin{proof}
    Iniziamo dimostrando il caso in cui $x_0$ sia uno degli estremi, ad esempio $a$. Il caso generale segue considerando separatamente il limite destro e sinistro. Dato che $x\to a$ possiamo restringerci ad un intorno in cui sia $g'(x)$ che $g(x)$ siano diversi da 0 (per la permanenza del segno).
    Supponiamo che $L$ sia finito. Per ogni $\epsilon\in(0,1)$ esiste un $x_1>a$ tale che $\forall\xi\in(a,b)$ risulta $$L-\epsilon<\frac{f'(\xi)}{g'(\xi)}<L+\epsilon$$ se $x\in(a,x_1)$, per il teorema di Cauchy esiste $\xi\in(x,x_1)$ tale che $$\frac{f(x)-f(x_1)}{g(x)-g(x_1)}=\frac{f'(\xi)}{g'(\xi)}$$
    e quindi $\forall x\in(a,x_1)$ $$L-\epsilon<\frac{f(x)-f(x_1)}{g(x)-g(x_1)}< L+\epsilon$$ D'altronde $$\frac{f(x)-f(x_1)}{g(x)-g(x_1)}=\frac{f(x)}{g(x)}\ \frac{1-\frac{f(x_1)}{f(x)}}{1-\frac{g(x_1)}{g(x)}}$$ 
    e quindi $$\frac{f(x)}{g(x)}=\frac{f(x)-f(x_1)}{g(x)-g(x_1)}\ \frac{1-\frac{g(x_1)}{g(x)}}{1-\frac{f(x_1)}{f(x)}}.$$
    Dato che $x\to a$ sia $f(x)$ che $g(x)$ tendono a $+\infty$ , la quantità: $$Q(x)=\frac{1-\frac{g(x_1)}{g(x)}}{1-\frac{f(x_1)}{f(x)}}$$
    tende ad $1$. Esisterà dunque un punto $x_2\leq x_1$ tale che $\forall x\in(a,x_2)$ si abbia $1-\epsilon<Q(x)<1+\epsilon$.\\ Di conseguenza (per la stima precedentemente fatta su $L\pm\epsilon$)
    $$(1-\epsilon)(L-\epsilon)z\frac{f(x)}{g(x)}<L+(L+\epsilon)(1+\epsilon).$$
    Per ogni $x\in(a,x_2)$ abbiamo $$(L+\epsilon)(1+\epsilon)=L+(L+1)\epsilon + \epsilon^2<L+(L+2)\epsilon$$
    e
    $$(L-\epsilon)(1-\epsilon)=L-(L+1)\epsilon + \epsilon^2<L-(L+2)\epsilon$$
    e quindi in conclusione
    $$L-(L+2)\epsilon<\frac{f(x)}{g(x)}<L+(L+2)\epsilon$$ e pertanto il rapporto $\frac{f(x)}{g(x)}$ tende ad $L$.
\end{proof}



Vediamo ora un risultato per certi versi sorprendente. Le funzioni derivabili non hanno necessariamente derivata continua (come testimoniato dalle funzioni di Weierstrass, si vedano le appendici relative del Buttazzo), ma nonostante ciò non ci vanno poi troppo lontano.

\begin{theorem}[Darboux] Le derivate di funzioni derivabili hanno la proprietà dei valori intermedi. 
\end{theorem}

\begin{proof} Supponiamo che $f:[a,b]\to\R$ sia una funzione derivabile e che si abbia $f'(a)\neq f'(b)$ (se questi valori coincidono non c'è alcunché da dimostrare). Senza perdita di generalità possiamo assumere $f'(b) > f'(a)$ e considerare un qualunque elemento $m\in(f'(a),f'(b))$. Proviamo ora che per qualche $\xi\in(a,b)$ si ha $f'(\xi)=m$. La funzione $g(x)=f(x)-mx$ è derivabile su $[a,b]$ e per Weierstrass ha un punto di minimo in $[a,b]$. Per il principio di Fermat tale punto non può essere collocato né in $a$ né in $b$, poiché $g'(a)=f'(a)-m < 0$ e $g'(b)=f'(b)-m > 0$. Ma allora, detta $\xi$ l'ascissa del punto di minimo, si hanno $\xi\in(a,b)$ e $g'(\xi)=0$, dove l'ultima identità è equivalente a $f'(\xi)=m$. 
\end{proof}


\subsection{Convessità}
\begin{definizione}
 Diciamo che un sottoinsieme $E\subseteq \R^n$ è \emph{convesso} se ogni coppia di punti di $E$ definisce un segmento interamente contenuto in $E$: 
 $$ \forall x,y\in E,\forall \lambda\in [0,1],\quad (1-\lambda)x+\lambda y \in E. $$
\end{definizione}
\begin{definizione}
 Dato un intervallo $I\subseteq\R$ e una funzione $f:I\to\R$, diciamo che $f$ è \emph{convessa} su $I$ se ha epigrafico convesso, ossia se soddisfa 
 $$ \forall x,y\in I,\forall \lambda\in[0,1],\quad f((1-\lambda)x + \lambda y) \leq (1-\lambda)f(x) + \lambda f(y) $$
 detta \emph{disuguaglianza di Jensen}.
\end{definizione}
\begin{definizione} $f:I\to\R$ è detta \emph{convessa per punti medi} se soddisfa la disuguaglianza di Jensen per $\lambda=\frac{1}{2}$.
\end{definizione}
\begin{lemma} Una $f:I\to\R$ è convessa se e solo se, per ogni $x,y,z\in I$ con $x\leq y\leq z$, si ha 
$$ \det \begin{pmatrix}1 & x & f(x) \\ 1 & y & f(y) \\ 1 & z & f(z) \end{pmatrix}\geq 0.$$
\end{lemma}
\begin{proof} Ogni $y\in[x,z]$ è combinazione convessa degli estremi e ogni combinazione convessa degli estremi è compresa tra questi. In particolare il Lemma è immediata conseguenza della riscrittura della disuguaglianza di Jensen per $y=(1-\lambda)x + \lambda z$. 
\end{proof}
\begin{cor} Ogni funzione convessa su $I$ è continua sulla parte interna di $I$. 
\end{cor}
\begin{cor} Data una $f$ convessa su $I$ è possibile definire su $\{(x,y)\in I^2: x\neq y\}$\\ la funzione \emph{rapporto incrementale} 
$$ \Delta(x,y) = \frac{f(y)-f(x)}{y-x} $$
e questa risulta debolmente crescente rispetto a ciascuno dei suoi parametri. Viceversa, una $f$ definita su $I$ è convessa solo se il corrispondente rapporto incrementale $\Delta$ è debolmente crescente nel precedente senso.
\end{cor}

\begin{cor} Se $f:I\to\R$ è derivabile e convessa, l'epigrafico di $f$ giace al di sopra di qualunque retta tangente, ossia 
$$ \forall x,y\in I,\qquad f(x)\geq f(y) + f'(y)(x-y).$$
\end{cor}

\begin{cor} Una $f:I\to\R$ derivabile è convessa se e solo se $f'$ è debolmente crescente su $I$. 
\end{cor}

\begin{cor} Una $f:I\to\R$ derivabile due volte è convessa se e solo se $f''\geq 0$.\end{cor}

\begin{theorem} Ogni $f:I\to\R$ continua e convessa per punti medi è convessa. 
\end{theorem}
\begin{proof} Ogni $y\in[x,z]$ può essere espresso come limite di una successione di combinazioni convesse $(1-\lambda_n)x+\lambda_n z$ dove $\lambda_n$ è un razionale diadico, ossia una frazione della forma $\frac{d}{2^m}$ con $d$ intero dispari e $m$ intero non negativo. Nelle ipotesi si ha pertanto che la disuguaglianza di Jensen per $\lambda=\frac{1}{2}$ comporta quella generale.
\end{proof}

Gli ultimi due risultati sono i principali strumenti per il riconoscimento della convessità di una funzione.\\
E la convessità è la madre delle principali disuguaglianze, sia in Analisi che in Algebra.

\begin{lemma}[Disuguaglianza di Jensen generalizzata] Se $f:I\to\R$ è convessa, per ogni $n$-upla $x_1,\ldots,x_n$\\ di elementi di $I$ e per ogni combinazione convessa di questi si ha 
$$ f\left(\sum_{k=1}^{n}\lambda_k x_k\right)\leq \sum_{k=1}^{n}\lambda_k f(x_k) $$
dove $\lambda_k\in[0,1]$ e $\sum_{k=1}^{n}\lambda_k = 1$. 
\end{lemma}
\begin{proof}
 È sufficiente procedere per induzione su $n$ componendo più istanze della disuguaglianza di Jensen in forma standard.
\end{proof}

\begin{lemma}[Disuguaglianza di Jensen in forma integrale] Se $\varphi:\R\to\R$ è convessa e $f$ è continua su $[a,b]$ si ha 
$$ \varphi\left(\frac{1}{b-a}\int_{a}^{b}f(x)\,dx\right) \leq \frac{1}{b-a}\int_{a}^{b}\varphi(f(x))\,dx.$$
\end{lemma}
\begin{proof} Segue dal risultato precedente interpretando gli integrali come limiti di somme di Riemann.\\ In Probabilità la disuguaglianza è spesso espressa in forma più concisa come $\varphi(\mathbb{E}[X])\leq \mathbb{E}[\varphi(X)]$. 
\end{proof}

\begin{ex}[AM-GM] Si dimostri che se $a_1,\ldots,a_n$ sono quantità positive si ha 
$$ \text{AM}\left(a_1,\ldots,a_n\right) = \frac{1}{n}\sum_{k=1}^{n}a_k \geq \sqrt[n]{\prod_{k=1}^{n}a_k} = \text{GM}\left(a_1,\ldots,a_n\right). $$ 
\end{ex}
\textbf{Soluzione}. Posto $a_k = \exp\left(b_k\right)$ si ha che la disuguaglianza in questione è la disuguaglianza di Jensen generalizzata per $f(x)=e^x$ e $\lambda_1=\ldots=\lambda_n=\frac{1}{n}$. In particolare è sufficiente invocare il fatto che $e^x$ è una funzione convessa, poiché continua e convessa per punti medi, o poiché positiva e coincidente con la propria derivata seconda.\qed

\begin{ex}[Cauchy-Schwarz] Si provi che per ogni $\vec{a},\vec{b}\in\R^n$ si ha $\langle \vec{a},\vec{b}\rangle^2 \leq \|\vec{a}\|^2\|\vec{b}\|^2$, ossia 
$$ \left(\sum_{k=1}^{n} a_k b_k\right)^2 \leq \sum_{k=1}^{n} a_k^2 \sum_{k=1}^{n} b_k^2. $$
\end{ex}
\textbf{Soluzione}. La distanza dall'origine è una funzione convessa in quanto ha per epigrafico un cono. Ne consegue che 
$$\forall \lambda\in[0,1],\qquad \|\lambda\vec{a}+(1-\lambda)\vec{b}\| \leq \lambda\|\vec{a}\|+(1-\lambda)\|\vec{b}\|. $$
Elevando al quadrato ambo i membri (certamente non negativi) abbiamo che 
$$ 2\lambda(1-\lambda)\langle\vec{a},\vec{b}\rangle\leq 2\lambda(1-\lambda)\|\vec{a}\|\|\vec{b}\|. $$
A meno di rimpiazzare $\vec{a}$ con $-\vec{a}$ possiamo certamente supporre che $\langle\vec{a},\vec{b}\rangle$ sia non negativo, alché la tesi segue da un ulteriore elevamento al quadrato.\qed 

\begin{ex}[Disuguaglianza tra le medie] Date $n$ quantità positive $a_1,\ldots,a_n$ definiamo la \emph{media di ordine $p>0$} di queste come 
$$ M_p\left(a_1,\ldots,a_n\right) = \text{AM}\left(a_1^p,\ldots,a_n^p\right)^{1/p}.$$
Si provi che se $p>q>0$ allora $M_p \geq M_q$. 
\end{ex}
\textbf{Soluzione}. A meno di opportune sostituzioni la disuguaglianza si rivela equivalente a $M_{p/q}\geq M_1$, che è immediata conseguenza della disuguaglianza di Jensen generalizzata per $f(x)=x^{p/q}$ (definita su $\R^+$) e $\lambda_1=\ldots=\lambda_n=\frac{1}{n}$.\qed



\begin{theorem}[Disuguaglianza di Hermite-Hadamard]
Se $f:[a,b]\to\mathbb{R}$ è una funzione convessa,
$$ f\left(\frac{a+b}{2}\right) \leq \frac{1}{b-a}\int_{a}^{b}f(x)\,dx \leq \frac{f(a)+f(b)}{2}.$$
\end{theorem}
\begin{proof} La convessità di $f$ comporta la sua continuità su $(a,b)$, da cui la sua Riemann-integrabilità. A meno di rimpiazzare $f(x)$ con $f(x)+c$ possiamo assumere senza perdita di generalità che si abbia $f\geq 0$ su $[a,b]$. Sempre per convessità, il sottografico di $f$ su $[a,b]$ è contenuto all'interno del trapezio avente vertici in $(a,0),(b,0),(b,f(b)),(a,f(a))$, da cui la maggiorazione.
E nuovamente per convessità, esiste una retta per $\left(\frac{a+b}{2},f\left(\frac{a+b}{2}\right)\right)$ che sull'intervallo $[a,b]$ giace al di sotto del grafico della funzione (se $f$ è derivabile nel punto medio dell'intervallo $[a,b]$, basta considerare la retta con coefficiente angolare $f'\left(\frac{a+b}{2}\right)$). L'area del trapezio delimitato dall'asse delle $x$, da $x=a$, da $x=b$ e dalla precedente retta ha area $(b-a)f\left(\frac{a+b}{2}\right)$, il che prova la minorazione.\end{proof}


\begin{ex} Si fornisca una decorosa approssimazione numerica del valore della serie $$\zeta(2)=\sum_{n\geq 1}\frac{1}{n^2}.$$
\end{ex}

\textbf{Soluzione}. La funzione $f(x)=\frac{1}{x^2}$ è convessa su $\mathbb{R}^+$. Per Hermite-Hadamard
$$ \frac{1}{n^2}\leq \int_{n-\frac{1}{2}}^{n+\frac{1}{2}}\frac{dx}{x^2}=\frac{1}{n-\frac{1}{2}}-\frac{1}{n+\frac{1}{2}} $$
vale per ogni $n\in\mathbb{N}^+$, da cui
$$\zeta(2)=\sum_{n\geq 1}\frac{1}{n^2}\leq 1+\int_{\frac{3}{2}}^{+\infty}\frac{dx}{x^2}=\frac{5}{3}, $$
che è una stima piuttosto accurata della serie.\qed



Oltre che nella fabbricazione di stime accurate, la convessità è di grande ausilio in molte altre circostanze: la \emph{Lemmata} conclusiva di questa sezione è lasciata come esercizio. Come prima, supponiamo tacitamente che $f:I\to\R$ sia convessa.\\



\textbf{Considerazioni di rilievo}. 
\begin{itemize}
 \item $f$ assume massimo negli estremi di $I$.
 \item Se $I$ è chiuso l'insieme dei punti di minimo di $f$ su $I$ è un convesso.\\ Più in generale, è convesso qualunque sottolivello non vuoto.
 \item Se $E\subset\R^n$ è un convesso compatto, è ben definita e continua la \emph{proiezione} su $E$ data da $\pi:\R^n\setminus E \to \partial E$,\\ che associa ad ogni $x\in\R^n\setminus E$ il punto di $E$ con la minima distanza da $x$.
 \item L'equazione $f(x)=\kappa$ ha al più due soluzioni distinte in $I$.
 \item Se $f$ è composta o sommata a qualunque altra funzione convessa resta convessa.
\end{itemize}

\subsection{Metodo di Newton}
Il concetto di convessità dal punto di vista geometrico è anche alla base di uno dei principali stratagemmi numerici per la determinazione di zeri di funzioni, il \emph{metodo delle tangenti} o \emph{metodo di Newton}.

\begin{theorem} Se $f\in C^2([a,b])$ è convessa, con derivata positiva e tale per cui $f(a)f(b)<0$, la successione definita attraverso 
$$ \xi_0 = b,\qquad \xi_{n+1} = \xi_n - \frac{f(\xi_n)}{f'(\xi_n)} $$
converge decrescendo all'unico zero di $f$ in $[a,b]$. 
 
\end{theorem}
\begin{proof}
Per convessità, il grafico di $f$ su $[a,b]$ è collocato al di sopra di qualunque retta tangente, in particolare si ha $f(x)\geq f'(\xi_n)(x-\xi_n)+f(\xi_n)$ per ogni $x\in[a,b]$. L'ultima retta tangente interseca l'asse delle ascisse in $\xi_{n+1}$, che si trova necessariamente alla sinistra di $\xi_n$ e alla destra dell'unico zero di $f$ in $[a,b]$. La successione $\{\xi_n\}_{n\geq 0}$, in quanto decrescente e inferiormente limitata, converge necessariamente al suo $\inf$, che denominiamo $\xi$. Da $\lim_{n\to +\infty}(\xi_{n+1}-\xi_n)=0$ segue $f(\xi)=0$.
\end{proof}

\textbf{Nota}: il metodo di Newton applicato alla funzione $f(x)=x^2-n$ è esattamente il metodo babilonese per l'approssimazione di $\sqrt{n}$, in cui si parte con un'approssimazione per eccesso $\alpha \geq \sqrt{n}$ e la si rimpiazza con la media aritmetica tra $\alpha$ ed $\frac{n}{\alpha}$, producendo una nuova approssimazione per eccesso, più piccola e più accurata della precedente.\\

Sotto le precedenti ipotesi di monotonia e convessità la convergenza è rapida, in particolare \emph{quadratica}. A meno di traslazioni non è restrittivo supporre che $0\in[a,b]$ sia l'unico zero di $f$, e dagli sviluppi di Maclaurin di $f(x)$ e della sua derivata, ossia $f(x)=f'(0)x + \frac{1}{2}(f''(0)+o(1))x^2$ e $f'(x)=f'(0)+(f''(0)+o(1))x$, si ha 
$$ \xi_{n+1} = \xi_n - \frac{f'(0)\xi_n + \frac{1}{2}(f''(0)+o(1))\xi_n^2}{f'(0)+(f''(0)+o(1))\xi_n} = O(\xi_n^2). $$


\newpage

\subsection{Teorema di Taylor e resto integrale}
Sia $I$ un intorno aperto dell'origine e $f$ una funzione di classe $C^d$ su $I$, con $d\geq 1$. Per un qualunque $t\in I$, possiamo considerare che l'applicazione della formula di integrazione produce quanto segue:

\begin{eqnarray*} \int_{0}^{t} (t-x)^{d-1} f^{(d)}(x)\,dx &=&\left[(t-x)^{d-1} f^{(d-1)}(x)\right]_{0}^{t}+(d-1)\int_{0}^{t}(t-x)^{d-2}f^{(d-1)}(x)\,dx\\ &=&-f^{(d-1)}(0)t^{d-1}+(d-1)\int_{0}^{t}(t-x)^{d-2}f^{(d-1)}(x)\,dx. \end{eqnarray*}
Applicando induzione su $d$ ne deduciamo:

$$ \int_{0}^{t} (t-x)^{d-1} f^{(d)}(x)\,dx = -\sum_{k=1}^{d-1}f^{(d-k)}(0) t^{d-k}(d-1)_k + (d-1)!\int_{0}^{t}f'(x)\,dx $$
dove $(d-1)_k$ indica il simbolo di Pochhammer decrescente, ossia $k!\binom{d-1}{k}$.\\ Dividendo ambo i membri per $(d-1)!$ abbiamo quanto segue:

$$ \frac{1}{(d-1)!}\int_{0}^{t}(t-x)^{d-1}f^{(d)}(x)\,dx = -\sum_{j=1}^{d-1}\frac{f^{(j)}(0)}{j!}\,t^j + f(t)-f(0). $$

Una immediata conseguenza è la \textbf{formula di Taylor con resto integrale} per funzioni di classe $C^d$ in un intorno dell'origine:
\begin{theorem}
 $$ f(t) = \sum_{j=0}^{d-1}\frac{f^{(j)}(0)}{j!}\,t^j + \frac{1}{(d-1)!}\int_{0}^{t}(t-x)^{d-1}f^{(d)}(x)\,dx. $$
\end{theorem}
Il primo addendo nel membro destro è detto \emph{polinomio di Taylor} di ordine $d-1$, ed è l'unico polinomio di grado $\leq (d-1)$ le cui derivate nell'origine, fino alla $(d-1)$-esima, coincidono con quelle di $f$.\\
Supponendo che $f^{(d)}(x)$ su $[0,t]$ assuma valori compresi tra $m$ ed $M$ abbiamo che il termine di resto è compreso tra $m$ volte ed $M$ volte la seguente quantità:
$$ \frac{1}{(d-1)!}\int_{0}^{t}(t-x)^{d-1}\,dx = \frac{t^d}{d!}. $$
Poiché le funzioni continue hanno la proprietà dei valori intermedi si ha
$$ f(t) = \sum_{j=0}^{d-1}\frac{f^{(j)}(0)}{j!}\,t^j + \frac{t^d}{d!}f^{(d)}(\xi)\quad\text{per qualche }\xi\in(0,t) $$
che è noto anche come \emph{sviluppo di Taylor con resto di Lagrange}. Sottolineiamo che la precedente identità continua a valere sotto ipotesi meno restrittive di quelle qui assunte: è sufficiente che $f^{(d)}$ esista, non è necessario che sia continua. Abbiamo comunque già visto che se $f^{(d)}$ esiste ha automaticamente la proprietà dei valori intermedi (MVP), e sebbene la MVP non comporti automaticamente la Riemann-integrabilità di $(t-x)^{d-1}f^{(d)}(x)$, nei casi concreti è assai raro imbattersi in funzioni che siano derivabili ma non abbiano derivata continua. Inoltre questa impostazione semplicemente basata su integrazione per parti e induzione ha il pregio di produrre una rappresentazione esplicita del resto ed evitare fastidiosi tecnicismi legati alla dimostrazione e all'applicazione del Teorema di De l'H\^opital.\\
Una forma ulteriormente indebolita del resto di Lagrange è data dal resto di Peano, dove la differenza tra $f(t)$ e il suo polinomio di Taylor di ordine $d-1$ è semplicemente espressa come $O(t^d)$. Nei casi in cui si voglia trattare uno sviluppo centrato in un punto $t_0$ diverso dall'origine è sufficiente traslare opportunamente la variabile $t$ e gli argomenti delle derivate di $f$.\\

\emph{Perché gli sviluppi di Taylor sono di fondamentale importanza in Analisi?} Perché permettono di ricondurre l'algebra delle funzioni sufficientemente regolari all'algebra dei polinomi. Nello specifico, perché permettono di
\begin{itemize}
 \item determinare limiti senza dover ricorrere a ripetuti confronti o applicazioni di De l'H\^opital
 \item rappresentare serie come integrali o viceversa (si veda a tal proposito l'esercizio $\textbf{J21}$)
 \item legare il comportamento delle derivate in un punto al comportamento locale o globale della funzione considerata. Il viceversa è ad esempio il problema-cardine in combinatoria analitica, ossia quello di ricostruire il comportamento asintotico di una successione unicamente dalle informazioni algebriche sulla classe combinatoria considerata. All'interno di questo punto rientra anche il problema-cardine della teoria analitica dei numeri, ossia quello di ricostruire la collocazione degli zeri (o dei poli) di certe funzioni a partire dalle equazioni differenziali o funzionali soddisfatte.
\end{itemize}

Prima di avventurarsi in Matematica raffinata è opportuno familiarizzare con i fondamentali, in questo frangente gli sviluppi di Maclaurin (ossia gli sviluppi di Taylor centrati nell'origine) delle funzioni di classe $C^{\infty}$ d'uso comune.

\begin{itemize}
 \item \textbf{Serie geometrica}. Per ogni $x\in(-1,1)$ si ha 
 $$ \frac{1}{1-x}=1+x+x^2+x^3+\ldots = \sum_{n\geq 0}x^n $$
 e in particolare $\left.\frac{d^n}{dx^n}\left(\frac{1}{1-x}\right)\right|_{x=0}=n!$. Rimpiazzando $x$ con $\pm x^d$ si ha che su $(-1,1)$ vale l'identità 
 $$ \frac{1}{1\pm x^d} = \sum_{n\geq 0}(\mp 1)^n x^{nd}. $$
 \item \textbf{Esponenziale, seno e coseno}. Sono funzioni già trattate nell'introduzione, che per ogni $x\in\R$ realizzano 
 $$ e^x = \sum_{n\geq 0}\frac{x^n}{n!},\quad \sin x=\sum_{n\geq 0}\frac{(-1)^n x^{2n+1}}{(2n+1)!}, \quad \cos x=\sum_{n\geq 0}\frac{(-1)^n x^{2n}}{(2n)!}. $$
 Rimuovendo le alternanze di segno nelle ultime due serie si ottengono gli sviluppi di Maclaurin di $\cosh x=\frac{e^x+e^{-x}}{2}$ e $\sinh x=\frac{e^x-e^{-x}}{2}$.
 \item \textbf{Potenze di $1\pm x$}. Dal calcolo esplicito delle derivate nell'origine di $(1+x)^{a}$ si ha che per ogni $x\in(-1,1)$ vale 
 $$ (1+x)^a = \sum_{k\geq 0}\frac{(a)_k}{k!} x^k\qquad\text{dove }(a)_k = a(a-1)\cdot\ldots\cdot (a-k+1) $$
 generalizzando il binomio di Newton a esponenti qualunque. Nel caso di esponenti interi e negativi l'identità 
 $$ \frac{1}{(1-x)^{m+1}} = \sum_{n\geq 0}\binom{n+m}{m} x^n $$
 può anche essere vista come conseguenza dell'espediente combinatorio noto come \emph{stars\&bars}.
 \item \textbf{Logaritmo naturale}. Su $(-1,1)$ abbiamo $\frac{d}{dx}\left(-\ln(1-x)\right)=\frac{1}{1-x}$. Per integrazione termine a termine della serie geometrica segue che sul precedente intervallo vale 
 $$ -\ln(1-x) = \sum_{n\geq 1}\frac{x^n}{n}. $$
 Per sommazione per parti (alias \emph{lemma di Abel}) l'identità continua a valere anche nell'estremo sinistro del dominio di convergenza, producendo $\ln 2 = \sum_{n\geq 1}\frac{(-1)^{n+1}}{n}$.
 \item \textbf{Arcotangente}. Vale $\frac{d}{dx}\arctan x=\frac{1}{1+x^2}$, che per $|x|<1$ può essere espressa come $\sum_{n\geq 0}(-1)^n x^{2n}$.\\
 Per integrazione termine a termine segue che su $(-1,1)$ vale 
 $$ \arctan(x) = \sum_{n\geq 0}\frac{(-1)^n}{2n+1} x^{2n+1}. $$
 Per sommazione per parti l'identità continua a valere anche in $x=1$,\\ producendo $\pi = 4\sum_{n\geq 0}\frac{(-1)^n}{2n+1}$ (\emph{serie di Gregory}).
 
\end{itemize}

Alcune considerazioni di simmetria hanno un certo rilievo anche nel contesto delle serie di Fourier: se una funzione di classe $C^\infty$ in un intorno dell'origine è dispari, le sue derivate nell'origine d'ordine pari sono tutte nulle. Analogamente, se una funzione di classe $C^\infty$ in un intorno dell'origine è pari, le sue derivate nell'origine d'ordine dispari sono tutte nulle. La dimostrazione è immediata: è sufficiente considerare l'identità $f(-x)=\pm f(x)$, derivare ambo i membri il corretto numero di volte e infine valutarli in $x=0$.\\

In questo contesto è bene non esagerare con l'ottimismo: per una generica funzione di classe $C^{\infty}$ in un intorno dell'origine, la successione delle derivate nell'origine in generale \textbf{non} fissa la funzione. Consideriamo $\exp(-x^2)$: non è difficile dimostrare che qualunque derivata ha limite zero per $x\to +\infty$ o $x\to -\infty$. Eseguendo la sostituzione $x\to \frac{1}{x}$ ne consegue che 
$$ g(x) = \left\{\begin{array}{rcl}\exp(-1/x^2)&\text{se}&x\neq 0\\ 0 &\text{se}&x=0\end{array}\right. $$
è una funzione di classe $C^\infty$ sull'intera retta reale che ha tutte le derivate nell'origine pari a zero. Tuttavia è palese che $g(x)$ non sia la funzione identicamente nulla, dunque che la successione costantemente nulla non sia associata ad un'unica funzione $C^\infty$ in un intorno dell'origine, ma ad un'intera classe di equivalenza di funzioni $C^\infty$, detta \emph{germe}.\\

Per nostra fortuna \emph{si può} recuperare una corrispondenza biunivoca tra alcune successioni ed alcune funzioni, restringendo opportunamente il grado di regolarità degli oggetti coinvolti. Diciamo che una funzione $f$ è \emph{analitica} (o di classe $C^\omega$) in un intorno $U$ dell'origine se è di classe $C^\infty$ su $U$ e per ogni $x\in U$ esiste una costante $\rho_x > 0$ tale da garantire definitivamente $|f^{(n)}(x)|\leq n! \rho_x^n$. Le quantità $\rho_x$ sono legate a quello che è il \emph{raggio di convergenza} di una serie di potenze, e la limitazione sul modulo delle derivate ci permette di guadagnare uniformità nel termine di resto dell'espansione di Maclaurin. Così facendo ogni successione ``che in modulo non cresce troppo in fretta'' (non più in fretta delle derivate nell'origine di $\frac{1}{1-Rx}$, per qualche $R>0$) risulta essere la successione delle derivate nell'origine di \emph{un'unica} funzione analitica in un intorno dell'origine. Tutto questo sarebbe poco più di una capriola concettuale se le funzioni d'impiego comune non fossero funzioni analitiche (ossia localmente coincidenti con la propria serie di Taylor), ma per fortuna \emph{lo sono}.

\begin{ex} Si determinino la derivata quarta e la derivata quinta nell'origine di $f(x)=\sin(x)\arcsin(x)$. 
\end{ex}
\textbf{Soluzione}. $f(x)$ è una funzione analitica su $(-1,1)$ e pari: in particolare $f^{(V)}(0)={\color{blue}0}$. Il valore di $f^{(IV)}(0)$ è fissato dallo sviluppo di Maclaurin al quart'ordine, ricostruibile a partire da $\sin(x)=x-\frac{x^3}{6}+o(x^4)$ e dallo sviluppo al quart'ordine di $\arcsin x$. Poiché $\frac{d}{dx}\arcsin x = (1-x^2)^{-1/2}$, da 
$$ \frac{1}{\sqrt{1-x}} = \sum_{n\geq 0}\binom{2n}{n}x^n $$
segue 
$$ \frac{1}{\sqrt{1-x^2}}= \sum_{n\geq 0}\binom{2n}{n} x^{2n} $$
e 
$$ \arcsin x = \sum_{n\geq 0}\frac{\binom{2n}{n}}{2n+1}x^{2n+1} = x + \frac{x^3}{6} + o(x^4).$$
Da quanto collezionato segue che al quart'ordine $\sin(x)\arcsin(x)$ è indistinguibile da $\left(x-\frac{x^3}{6}\right)\left(x+\frac{x^3}{6}\right)=x^2+o(x^4)$, dunque anche il valore di $f^{(IV)}(0)$ è $\color{blue}0$.

\newpage

\section{Binomiali \& Friends}
Il coefficiente binomiale $\binom{n}{k}$, letto ``\emph{$n$ su $k$}'', rappresenta il numero di sottoinsiemi di cardinalità $k$ in un insieme di $n$ elementi.
Questo è esplicitamente:\\
$$\binom{n}{k} = \frac{n!}{(n-k)!k!}.$$
Proprietà dei binomiali, di immediata dimostrazione dalla definizione combinatoria, sono:
\begin{itemize}
    \item $\binom{n}{k}=\binom{n}{n-k}$ (ci sono tanti modi di scegliere $k$ elementi tra $n$ quanti modi di \emph{non} scegliere $n-k$ elementi tra $n$)
    \item $\binom{n+1}{k+1}=\binom{n}{k+1} + \binom{n}{k}$ (distinguendo i sottoinsiemi che contengono $n$ da quelli che non lo contengono)
    \item $2^n= \sum_{k=0}^{n}\binom{n}{k}$ (i sottoinsiemi totali sono quelli che sono)
    \item $0 = \sum_{k=0}^{n}\binom{n}{k}(-1)^k$ (ci sono tanti sottoinsiemi di cardinalità pari quanti sottoinsiemi di cardinalità dispari)
\end{itemize}

\subsection{Teorema Binomiale di Newton e applicazioni utili}
L'identità nota come \emph{binomio di Newton} è immediata conseguenza della proprietà distributiva:
$$(a+b)^n=\sum_{k=0}^{n}\binom{n}{k}a^{n-k}b^k.$$
Uno dei meriti di Newton è stato provare che $(a+b)^m=\sum_{k\geq 0}\binom{m}{k}a^{m-k}b^k$ continua a valere anche per $m$ non intero, ridefinendo opportunamente $\binom{m}{k}$.

\subsection{Stars \& Bars}
Un importante tecnica di conteggio è appunto \textbf{Stars \& Bars}:
$$\left|\{(x_1,\ldots,x_k)\in(\N^+)^k: x_1+\ldots+x_k = n\}\right|=[x^n]\left(\frac{x}{1-x}\right)^k = \binom{n-1}{k-1},$$
$$ \frac{1}{(1-x)^{k+1}}=\sum_{n\geq 0}\binom{n+k}{k}x^n. $$
L'interpretazione grafica è data dal rispondere alla domanda: in quanti modi possiamo mettere $k-1$ barrette nelle intercapedini tra $n$ oggetti adiacenti? Eccone anche un'applicazione ad un esercizio già visto, ossia la determinazione di $\sum_{n\geq 0}\frac{n^2}{4^n}$. Abbiamo in primo luogo che $n^2$ è certamente una combinazione lineare di $\binom{n+2}{2},\binom{n+1}{1}$ e $\binom{n}{0}=1$.\\ In particolare, per eliminazione gaussiana (o differenze in avanti) vale $n^2=2\binom{n+2}{2}-3\binom{n+1}{1}+\binom{n}{0}$.\\ Da stars\&bars segue allora che 

$$ \sum_{n\geq 0} n^2 x^n = \frac{2}{(1-x)^3}-\frac{3}{(1-x)^2}+\frac{1}{1-x} $$

e valutando ambo i membri in corrispondenza di $x=\frac{1}{4}$ si ha immediatamente 

$$ \sum_{n\geq 0}\frac{n^2}{4^n} = 2\left(\frac{4}{3}\right)^3-3\left(\frac{4}{3}\right)^2+\left(\frac{4}{3}\right)=\frac{20}{27}.$$
\\

\subsection{Binomiale Centrale}
I coefficienti binomiali centrali sono i binomiali del tipo:
$$\binom{2n}{n}=\frac{(2n)!}{n!^2}=\frac{2^n(2n-1)!!}{n!}$$
così chiamati perché occupano le posizioni centrali del triangolo di Tartaglia.\\ I binomiali centrali figurano nell'\emph{identità di Vandermonde}
$$ \sum_{k=0}^{n}\binom{n}{k}^2 = \sum_{k=0}^{n}\binom{n}{k}\binom{n}{n-k}=[x^n](1+x)^n (1+x)^n = [x^n](1+x)^{2n} = \binom{2n}{n} $$
ed hanno un ruolo chiave nella dimostrazione di Chebyshev della forma debole del Teorema dei numeri primi,\\ $\pi(x)\frac{x}{\ln x}\in [c_1,c_2]$. Seguono disuguaglianze utili per la stima dei binomiali centrali:
    \begin{itemize}
        \item $\frac{4^n}{n+1}\leq\binom{2n}{n}\leq4^n$
        \item $\frac{4^n}{2\sqrt{n}}\leq\binom{2n}{n}\leq\frac{4^n}{\sqrt{3n+1}}$
    \end{itemize}
e ancora più accuratamente $\frac{1}{4^n}\binom{2n}{n}\sim\frac{1}{\sqrt{\pi n}}$ con $\frac{1}{4^n}\binom{2n}{n}\leq \frac{1}{\sqrt{\pi\left(n+\frac{1}{4}\right)}}$.\\ Queste possono essere ricavate dalla rappresentazione integrale 
$$ \frac{1}{4^n}\binom{2n}{n} = \frac{2}{\pi}\int_{0}^{\pi/2}\left(\cos\theta\right)^{2n}\,d\theta. $$
    
\subsection{Hockey Stick Identity}
Vale l'identità:
$$\binom{n+1}{r+1}=\sum_{j=r}^{n}\binom{j}{r}$$
\begin{proof}
 Fissato $r$, è sufficiente procedere per induzione su $n$. In alternativa, è sufficiente catalogare i sottoinsiemi di taglia $r+1$ di $\{1,2,\ldots,n+1\}$ in base al loro elemento massimo.
\end{proof}
Dalla HSI è semplice ricostruire le formule per la somma di potenze consecutive, una cui immediata conseguenza è ad esempio $\int_{0}^{t}x^n\,dx = \frac{t^{n+1}}{n+1}$ per ogni $t\in\R$ ed ogni $n\in\N$. Vediamo il caso $n=3$:
\begin{ex}[Identità di Nicomaco] Si determini esplicitamente, in funzione di $N$, il valore della somma 
$$ \sum_{n=1}^{N} n^3. $$
 
\end{ex}
\textbf{Soluzione}. Osserviamo che $n^3$ è un polinomio di grado $\leq 3$ della variabile $n$, così come $\binom{n}{k}$ per $k\in[0,3]$.\\ In particolare $n^3$ è una combinazione lineare a coefficienti costanti dei precedenti binomiali, $n^3=\sum_{k=0}^{3}\binom{n}{k}c_k$.\\
I valori dei coefficienti $c_k$ possono essere ricostruiti per eliminazione, o più efficientemente per differenze in avanti:
$$\begin{array}{ccccccc} 0 && 1 && 8 && 27 \\ 
& 1 && 7 && 19 & \\
&& 6 && 12 && \\
&&& 6 &&&
\end{array} $$
fornisce $n^3=6\binom{n}{3}+6\binom{n}{2}+1\binom{n}{1}$ (i coefficienti così trovati sono legati ai \emph{numeri di Stirling del secondo tipo}).\\ Dalla HSI abbiamo pertanto
$$ \sum_{n=1}^{N}n^3 = 6\sum_{n=1}^{N}\binom{n}{3}+6\sum_{n=1}^{N}\binom{n}{2}+\sum_{n=1}^{N}\binom{n}{1}=6\binom{N+1}{4}+6\binom{N+1}{3}+\binom{N+1}{2}={\color{blue}\left(\frac{N(N+1)}{2}\right)^2}=\left(\sum_{n=1}^{N}n\right)^2.$$


    \section{Criteri e metodi per Serie Numeriche}
    \subsection{Serie importanti}
Seguono le più importanti Serie Numeriche convergenti, utili nella stima di altre serie e nella verifica della loro convergenza.\\
\textbf{Serie armonica generalizzata}
$$\sum_{n=0}^{\infty}\frac{1}{n^\alpha}$$
è una serie convergente se e solo se
$\alpha\in\R>1$
\\
\textbf{Serie geometrica}
$$S=\sum_{n=0}^{\infty}x^n$$
Ha il seguente comportamento:
\begin{equation*}
\begin{cases}
    |x|\in(0,1) \rightarrow S=\frac{1}{1-x}
    \\x \geq 1 \rightarrow S=\infty
    \\x\leq -1 \rightarrow \not\exists S
    
\end{cases}
\end{equation*}
\textbf{Serie telescopica}
Ogni somma/serie in cui il termine generale si può esprimere come differenza di termini consecutivi (o quasi) si calcola, infatti vale
$$\sum_{n=0}^{N}\left(a_n - a_{n+1}\right) = a_0 - a_{N+1}$$
per massiccia cancellazione: il membro sinistro corrisponde alla differenza tra la prima quantità sommata e l'ultima quantità sottratta.

\subsection{Criteri}
Seguiranno i principali criteri di convergenza per le serie.
\subsubsection{Serie a termini definitivamente non negativi}
\textbf{Criterio del Confronto} \\
Sia
$$\sum_{n=0}^{\infty}a_n<\infty$$
Se vale che definitamente $0\leq b_n\leq a_n$, allora
$$\sum_{n=0}^{\infty}b_n<\infty.$$
Eventualmente in combinazione con somme telescopiche, questo criterio prova la convergenza o divergenza di molte serie. Ad esempio da $\frac{1}{n^2}<\frac{1}{n(n-1)}$ segue 
$$ \sum_{n\geq 1}\frac{1}{n^2} = 1+\sum_{n\geq 2}\frac{1}{n^2} \leq 1+\sum_{n\geq 2}\left(\frac{1}{n-1}-\frac{1}{n}\right) = 2 $$
e un discorso analogo può essere applicato anche alla serie armonica generalizzata.\\

\textbf{Criterio del Confronto asintotico} \\
È una forma più generale del precedente criterio. Se due successioni a termini non negativi $\{a_n\}_{n\geq 1}$ e $\{b_n\}_{n\geq 1}$ \\sono tali per cui $\lim_{n\to+\infty}\frac{a_n}{b_n}$ esiste finito, le serie 
$$ \sum_{n\geq 1} a_n,\qquad \sum_{n\geq 1} b_n $$
sono entrambe convergenti o entrambe positivamente divergenti.\\

\textbf{Criterio del Rapporto} \\
Sia
$$\sum_{n=0}^{\infty}a_n$$
una serie a termini mai nulli. Se si ha che:
$$\lim_{n\to\infty} \left|\frac{a_{n+1}}{a_n}\right|=c<1$$
allora la serie degli $a_n$ converge assolutamente.\\
È una conseguenza abbastanza immediata della definizione di limite e del criterio del confronto.\\ O, volendo, del criterio successivo, in quanto per il Lemma di Hadamard $\frac{|a_{n+1}|}{|a_n|}\to c$ comporta $\sqrt[n]{|a_n|}\to c$.\\

\textbf{Criterio della Radice} \\
Se $\{a_n\}_{n\geq 1}$ è una successione a valori non negativi tale per cui
$$ \lim_{n\to +\infty}\sqrt[n]{a_n} = c \in [0,1), $$
allora la serie $\sum_{n\geq 1}a_n$ converge.\\
È una conseguenza immediata del confronto con una serie geometrica convergente.

\subsubsection{Serie a termini di segno variabile}

\textbf{Criterio di Leibniz}\\
Se $\{a_n\}_{n\geq 0}$ è una successione decrescente a zero, la serie
$$ \sum_{n\geq 0} (-1)^n a_n $$
è convergente.\\
\begin{proof} Posto come di consueto $A_N=\sum_{n=0}^{N}a_n$, dalle ipotesi segue che $\{A_{2N}\}_{N\geq 0}$ è una successione decrescente mentre $\{A_{2N+1}\}_{N\geq 0}$ è una successione crescente. Poiché $A_{2N} > A_{2N+1}$ le due successioni sono entrambe convergenti, in quanto quella crescente è limitata dall'alto mentre quella decrescente è limitata dal basso. Inoltre i limiti di queste due successioni debbono coincidere, in quanto $A_{2N+1}-A_{2N}=a_{2n+1}$ converge a zero per ipotesi. Segue che l'intera successione delle somme parziali è convergente, ossia che la serie è convergente. 
\end{proof}


Una importante generalizzazione è data dal \textbf{Criterio di (Abel-)Dirichlet}.\\
Se $\{a_n\}_{n\geq 0}$ è una successione con somme parziali limitate e $\{b_n\}_{n\geq 0}$ è una successione decrescente a zero,\\ la serie $\sum_{n\geq 0}a_n b_n$ è convergente.

\emph{Sketch della dimostrazione}. Segue dall'identità
$$ \sum_{n=0}^{N}a_n b_n = A_N b_N - \sum_{n=0}^{N-1} A_n (b_{n+1}-b_n) $$
che è un analogo discreto della formula di integrazione per parti, nota come \textbf{formula di sommazione per parti}.\\ La sua dimostrazione è banale per induzione su $N$.\qed

Dalla formula di integrazione per parti segue l'omonimo criterio per gli integrali: se su $[0,+\infty)$ si ha che $f(x)$ ha primitiva limitata e $g(x)$ è decrescente a zero, $f(x)g(x)$ è impropriamente Riemann-integrabile su $\R^+$.

\subsection{Serie di Potenze}
Una serie di potenze (centrata nell'origine) è definita come:
$$\sum_{n=0}^{\infty}a_n x^n$$
con $x\in\C$, e sia il \textbf{Raggio di Convergenza} \\
$$\rho=\frac{1}{\limsup\sqrt[n]{|a_n|}}.$$
La serie di potenze converge puntualmente nella palla $B_0=\left\{z\in\C : |z|< \rho \right\}$ e uniformemente su ogni compatto contenuto in $B_0$.

\subsection{Metodi creativi per il calcolo esplicito}
Vediamo adesso come abusare della linearità delle Serie per calcolare esplicitamente una classe di queste. \\
Sia
$$S=\sum_{n\geq 0}^{}\frac{f(n)}{k^n}$$
con $k\in\N \wedge k\geq 2$.
Sarebbe molto comodo in questo caso ricondursi ad una serie geometrica.
Ipotizzando che esista una applicazione lineare e continua $T$ tale da mandare $x^n$ in $f(n)$ otteniamo:
$$S=\sum_{n\geq 0}\frac{f(n)}{k^n}=\sum_{n\geq 0}\frac{T(x^n)}{k^n}= T\left(  \sum_{n\geq 0}\left(\frac{x}{k}\right)^n \right)= T \left(\frac{1}{1-\frac{x}{k}}\right)$$ \\
in base alla scelta/costruzione di $T$ possiamo calcolare esplicitamente tute le serie di questo tipo.
(Possibili applicazioni lineari che farebbero al caso nostro sono: derivazione, integrazione, somma, prodotto, valutazione e tutte le loro composizioni). 
Un esempio di utilizzo di questo metodo è calcolare esplicitamente la serie:
$$\sum_{n\geq 0}^{}\frac{n^2}{4^n}$$
utilizzando come operatore $T$ la somma delle prime due derivate valutata in $1$ (esercizio per il lettore).\\
In generale questo metodo è utile ogni qual volta si sta trattando una serie i cui termini, a meno di un fattore, coincidono con quelli di una serie che già sappiamo calcolare.\\
A onor del vero, per quanto potente è raro che tale metodo sia \emph{l'unico} in grado di esplicitare un'assegnata serie. 
Tornando all'esempio precedente, è ovvio che la serie presentata sia convergente, in quanto definitivamente $\frac{n^2}{4^n}\in\left(0,\frac{1}{3^n}\right]$. 
Sono allora lecite tutte le seguenti manipolazioni (basate unicamente su moltiplicazioni per $4$ e \emph{reindexing}):
$$ S=\sum_{n\geq 0}\frac{n^2}{4^n}=\sum_{n\geq 1}\frac{n^2}{4^n},\qquad 4S=\sum_{n\geq 1}\frac{n^2}{4^{n-1}}=\sum_{m\geq 0}\frac{(m+1)^2}{4^m}, $$

$$ 3S=4S-S = \sum_{n\geq 0}\frac{(n+1)^2-n^2}{4^n}=\sum_{n\geq 0}\frac{2n+1}{4^n}=1+\sum_{n\geq 1}\frac{2n+1}{4^n},$$

$$ 12S=4(3S) = 4+\sum_{n\geq 1}\frac{2n+1}{4^{n-1}} = 4+\sum_{m\geq 0}\frac{2m+3}{4^m}, $$

$$ 9S = 12S-3S = 4+\sum_{n\geq 0}\frac{(2n+3)-(2n+1)}{4^n} = 4+2\sum_{n\geq 0}\frac{1}{4^n} = 4+\frac{2}{1-\frac{1}{4}} = 4+\frac{8}{3} = \frac{20}{3} $$
dalle quali segue $S=\frac{20}{27}$.


\subsubsection{Criterio Cannonata}

Per il \textbf{Criterio di Convergenza Dominata} tutte le serie che convergono \emph{abbastanza} velocemente commutano con l'integrale. All'atto pratico ciò comporta che, nelle ipotesi corrette, è lecito scrivere:
$$\sum_{n\geq 0}^{}\int_{a}^{b}f(x)dx =\int_{a}^{b}\sum_{n\geq 0}^{}f(x) dx.$$ 
Questo torna utile in quanto è moderatamente comune sostituire ad una funzione una sua \textbf{Rappresentazione integrale}.
(Non esattamente collegato, ma per farsi un'idea dell'utilità di questo fatto vedere l'esercizio \textbf{J17})
\subsection{Creative telescoping}
Un metodo importante è quello di trasformare serie apparentemente complesse in serie telescopiche o combinazioni lineari di serie telescopiche.
Tipicamente per questo tipo di lavoro bisogna farci l'occhio e sapere un po' dove mettere le mani, ma diventa abbastanza intuitivo velocemente (serve appunto un po' di creatività). Un esempio classico riguarda il problema di Basilea, ossia la determinazione di $\zeta(2)=\sum_{n\geq 1}\frac{1}{n^2}$. Il termine generale della serie non è evidentemente telescopico, ma è prossimo a quello della serie di Mengoli, da cui:
$$ \sum_{n\geq 1}\frac{1}{n^2} = \sum_{n\geq 1}\frac{1}{n(n+1)}+\sum_{n\geq 1}\frac{1}{n^2(n+1)} = 1+\sum_{n\geq 1}\frac{1}{n^2(n+1)}. $$
Alla ``serie residua'' che costituisce l'ultimo termine possiamo applicare una manipolazione analoga, eventualmente scorporando il primo addendo. Ciò conduce a 
$$ \zeta(2) = 1+\sum_{n\geq 1}\frac{1}{n(n+1)(n+2)}+2\sum_{n\geq 1}\frac{1}{n^2(n+1)(n+2)} = 1+\frac{1}{4}+2\sum_{n\geq 1}\frac{1}{n^2(n+1)(n+2)}. $$
Iterando il processo si ottiene l'identità 
$$ \zeta(2)=\sum_{n\geq 1}\frac{1}{n^2} = 3\sum_{n\geq 1}\frac{1}{n^2\binom{2n}{n}}$$
in cui le somme parziali dell'ultima serie convergono sensibilmente più in fretta delle somme parziali della serie originaria, poiché $\binom{2n}{n}\sim \frac{4^n}{\sqrt{\pi n}}$. Parliamo in questi casi di \emph{accelerazione} di serie convergenti. Dalle rappresentazioni integrali dei binomiali centrali abbiamo inoltre che l'ultima identità comporta
$$ \zeta(2) = \frac{3}{2}\int_{0}^{1}\sum_{m\geq 0}\frac{(x(1-x))^{m}}{m+1}\,dx = \frac{3}{2}\int_{0}^{1}-\log(1-x+x^2)\left(\frac{1}{x}+\frac{1}{1-x}\right)\,dx \stackrel{\text{sym}}{=}3\int_{0}^{1}\frac{-\log\Phi_6(x)}{x}\,dx $$
o, via integrazione per parti, $\zeta(2)=3\int_{0}^{1}\frac{2x-1}{1-x+x^2}\log(x)\,dx$. Sebbene Eulero non abbia seguito questa strada, e sebbene sia tecnicamente più semplice applicare l'identità di Parseval a serie di Fourier elementari, da qui si può giungere senza atroci intoppi a provare che $\zeta(2)=\frac{\pi^2}{6}$. La fabbricazione di serie accelerate, in cui il termine generale abbia decorose proprietà aritmetiche, riveste una certa importanza anche in \emph{approssimazione diofantea}, in quanto permette in alcune circostanze di dimostrare l'irrazionalità di fissate costanti. Apery si è ad esempio servito di 
$$ \zeta(3) = \frac{5}{2}\sum_{n\geq 1}\frac{(-1)^{n+1}}{n^3\binom{2n}{n}} $$
per dimostrare nel 1977 che $\zeta(3)\not\in\Q$.\\ La presunta irrazionalità di $\zeta(5),\gamma,\Gamma\left(\frac{1}{5}\right)$ (e molte altre costanti) è tuttora una questione aperta.



\newpage
\section{Integrazione secondo Riemann}
Questo paragrafo, dopo un breve recap, si concentrerà nell'illustrare degli integrali più complessi e meritevoli di particolare attenzione, oltre che a tecniche di integrazione un po' più oscure. Ad avviso mio e di Jack, i metodi che mostreremo sono abbastanza importanti da avere a portata di mano in caso di necessità. Alcuni argomenti che saranno trattati sono l'utilizzo della convergenza dominata (vedere paragrafo sui criteri delle serie), Integrali complessi, \emph{abuso} di rappresentazioni integrali etc.
\begin{definizione}[Partizione] Dato un intervallo chiuso e limitato della retta reale $[a,b]$, una qualunque sequenza 
$$ a=a_0 < a_1 < \ldots < a_n = b $$
definisce una \textbf{partizione} di $[a,b]$:
$$ [a,b]=\bigcup_{k=1}^{n} I_k,\qquad I_k=[a_{k-1},a_{k}]. $$ 
\end{definizione}
\begin{definizione}[Calibro o finezza] Il \emph{calibro} o \emph{finezza} di una partizione è la massima lunghezza degli intervalli che costituiscono la partizione. 
\end{definizione}
\begin{definizione}[Ordinamento parziale delle partizioni] Date due partizioni $\mathcal{P}_1$ e $\mathcal{P}_2$ dello stesso intervallo, diciamo che $\mathcal{P}_1$ \emph{è più fine} di $\mathcal{P}_2$, in simboli $\mathcal{P}_1 \geq \mathcal{P}_2$, se gli estremi degli intervalli di $\mathcal{P}_1$ costituiscono una sottosequenza degli estremi degli intervalli di $\mathcal{P}_2$. Questo definisce una relazione d'ordine parziale sull'insieme delle partizioni: date due distinte partizioni $\mathcal{P}_1,\mathcal{P}_2$, non è detto che $\mathcal{P}_1$ sia più fine di $\mathcal{P}_2$ o viceversa. Tuttavia, date due distinte partizioni $\mathcal{P}_1,\mathcal{P}_2$, esiste sempre una partizione $\mathcal{P}_3$ più fine di entrambe: è sufficiente prendere gli estremi degli intervalli di $\mathcal{P}_3$ come quelle quantità che sono un estremo di un intervallo di $\mathcal{P}_1$ o (inclusivo) di $\mathcal{P}_2$.
\end{definizione}
L'\textbf{integrale di Riemann} in senso proprio si definisce a partire da $f:[a,b]\to\R$ limitate.

\begin{definizione}[Somme di Riemann superiori e inferiori]
Data una $f$ siffatta e una partizione $\mathcal{P}=\bigcup_{k=1}^{n}I_k$ di $[a,b]$, definiamo la \textbf{somma di Riemann superiore} $S^+_{\mathcal{P}}(f)$ e la  \textbf{somma di Riemann inferiore} $S^-_{\mathcal{P}}(f)$ nel seguente modo:

$$ S^+_{\mathcal{P}}(f) = \sum_{k=1}^{n} \mu(I_k) \sup_{x\in I_k} f(x), \qquad S^-_{\mathcal{P}}(f) = \sum_{k=1}^{n} \mu(I_k) \inf_{x\in I_k} f(x)$$
dove $\mu(I_k)$ denota la lunghezza di $I_k$, ossia $a_{k}-a_{k-1}$.
\end{definizione}

Seguono alcuni lemmi di immediata dimostrazione ma di cruciale importanza:
\begin{itemize}
 \item Per qualunque partizione $\mathcal{P}$ si ha $S^+_{\mathcal{P}}\geq S^-_{\mathcal{P}}$
 \item Se $\mathcal{P}_2$ è più fine di $\mathcal{P}_1$, $S^+_{\mathcal{P}_2} \leq S^+_{\mathcal{P}_1}$
 \item  Se $\mathcal{P}_2$ è più fine di $\mathcal{P}_1$, $S^-_{\mathcal{P}_2} \geq S^+_{\mathcal{P}_1}$
\end{itemize}

\begin{definizione}[Integrale di Riemann]
I precedenti lemmi comportano l'esistenza e la finitezza sia di $\sup_{\mathcal{P}}S^-_{\mathcal{P}}$, detto \emph{integrale di Riemann inferiore}, che di $\inf_{\mathcal{P}}S^+_{\mathcal{P}}$, detto \emph{integrale di Riemann superiore}. È automatico che l'integrale di Riemann inferiore sia $\leq$ dell'integrale di Riemann superiore, ma non è affatto automatico che tali quantità coincidano. Se coincidono, $f$ è detta \emph{Riemann-integrabile} su $[a,b]$ e il valore di $\sup_{\mathcal{P}}S^-_{\mathcal{P}}=\inf_{\mathcal{P}}S^+_{\mathcal{P}}$ è denotato come 
$$ \int_{a}^{b} f(x)\,dx. $$
\end{definizione}

\begin{theorem} Ogni $f\in C^0([a,b])$ è Riemann-integrabile su $[a,b]$ e realizza 
$$ \int_{a}^{b} f(x)\,dx = \lim_{N\to +\infty}\frac{b-a}{N}\sum_{k=1}^{N}f\left(a+\frac{k}{N}(b-a)\right) = \lim_{N\to +\infty}\frac{b-a}{N}\sum_{k=0}^{N-1}f\left(a+\frac{k}{N}(b-a)\right).$$
\end{theorem}
\begin{proof} Per Heine-Cantor $f$ è uniformemente continua su $[a,b]$, ossia per ogni $\epsilon > 0$ esiste un $\delta > 0$ che assicura 
$$ |x_1-x_2|\leq \delta\quad\Longrightarrow\quad |f(x_1)-f(x_2)|\leq \varepsilon.$$
Per qualunque partizione $\mathcal{P}$ di calibro $\leq \delta$ si ha pertanto che 
$$ S^+_{\mathcal{P}}- S^-_{\mathcal{P}} = \sum_{k=1}^{n}\mu(I_k)\left(\sup_{x\in I_k}f(x)-\inf_{x\in I_k}f(x)\right)= \sum_{k=1}^{n}\mu(I_k)\left(\max_{x\in I_k}f(x)-\min_{x\in I_k}f(x)\right)\leq \sum_{k=1}^{n}\mu(I_k)\epsilon = \epsilon (b-a). $$
Data l'arbitrarietà di $\epsilon$ si ha che l'integrale di Riemann inferiore e l'integrale di Riemann superiore hanno distanza arbitrariamente piccola, cioè nulla, visto che $\R$ è un campo archimedeo, dunque privo di infinitesimi. Ciò comporta la Riemann-integrabilità di $f$. L'uniforme continuità di $f$ comporta anche che l'integrale $\int_{a}^{b} f(x)\,dx$ coincida con il $\sup_{\mathcal{P}\in U} S^-_{\mathcal{P}}$ e con l'$\inf_{\mathcal{P}\in U} S^+_{\mathcal{P}}$ sull'insieme $U$ delle \emph{partizioni uniformi}, ossia quelle in cui tutti gli intervalli hanno lunghezza pari al calibro. All'interno dell'insieme di queste partizioni, nuovamente per uniforme continuità di $f$, sia 

$$ S^+_{\mathcal{P}}-\sum_{k=1}^{n}\mu(I_k) f(\max I_k)\quad\text{che}\quad S^-_{\mathcal{P}}-\sum_{k=1}^{n}\mu(I_k) f(\min I_k) $$
possono essere resi arbitrariamente vicini a zero in modulo. Questo prova le ultime due uguaglianze. 
\end{proof}
\begin{definizione}
    Sia $X$ un intervallo di $\R$, e sia $f:X\to\R$ una funzione. Si dice che $F:X\to\R$ è una primitiva di $f$ in $X$ se per ogni $x\in X$ $F$ è derivabile e $F'(x)=f(x)$ e si denota con $$\int f = F$$
\end{definizione}
\begin{theorem}
Le primitive di funzioni integrabili secondo Riemann godono delle seguenti proprietà:
\begin{itemize}
    \item $$\int_{\R} (f+g)= \int_{\R} f + \int_{\R} g $$
    \item $$\forall \alpha\in\R,\quad \int_{\R}(\alpha f)=\alpha\int_{\R} f $$
    \item $$f\leq g \Longrightarrow \int_{\R}f\leq \int_{\R}g$$
\end{itemize}
Questo si traduce nel fatto che l'operatore $\int$ è \emph{lineare} e \emph{monotono}.
\end{theorem}
\begin{theorem}
    Sia $[a,b]\in\R$ un intervallo e sia $f:[a,b]\in\R$ Riemann integrabile in $[a,b]$ e sia $c\in[a,b]$ fissato.\\ Vale che:\\
    1) La funzione integrale di $f$ di punto iniziale $c$, $\int_c^x f(t)\,dt$ è localmente lipschitziana e quindi continua in $[a,b]$ \\
    2) (Torricelli) Se $f$ è continua in $x\in[a,b]$ allora $\int_c^x f(t)\,dt$ è derivabile in $x$ e si ha: $$\left(\int_c^x f(t)\,dt\right)' =f(x)$$
\end{theorem}
\begin{proof}
    Iniziamo a dimostrare la continuità. Sia $[a,b]$ un intervallo fissato, sia $\lambda=\sup_{x\in[a,b]}|f(x)|$.\\ Mostriamo che $\lambda$ è la costante di Lipschitz cercata su $[a,b]$:
    $$\left|\int_c^xf(x_2)\,dx-\int_c^xf(x_1)\,dx\right|=\left|\int_{x_1}^{x_2}f(t)\,dt\right|\leq\left|\int_{x_1}^{x_2}|f(t)|\,dt\right|\leq\left|\int_{x_1}^{x_2}\lambda dt\,\right|=\lambda|x_2-x_1|$$
    Introduciamo per semplicità la notazione: $\int_c^xf(t)\,dt= I_c f(x).$ \\
    Per quanto riguarda la parte dovuta a Torricelli bisogna provare che: 
    $$\lim_{h\to x}\left|\frac{I_cf(h)-I_cf(x)}{h-x}-f(x)\right|=0.$$
    Bisogna quindi dimostrare che fissato $\epsilon>0$ esiste $\delta>0$ tale che:
    $$\left|\frac{I_cf(h)-I_cf(x)}{h-x}-f(x)\right|\leq\epsilon\quad \text{ se }  0<|h-x|\leq\delta, h\in[a,b]$$
    $I_cf(h)-I_cf(x)$ si può scrivere come $\int_x^hf(t)dt$, inoltre 
    $$f(x)=f(x)\cdot 1 = f(x)\left(\frac{1}{h-x}\int_x^h dt\right)=\frac{1}{h-x}\int_x^h f(x)\,dt.$$
    Essendo $f(x)$ costante rispetto alla variabile di integrazione $t$ si ha che:
    $$\left|\frac{I_cf(h)-I_cf(x)}{h-x}-f(x)\right|=\left|\frac{1}{h-x}\int_x^h f(t)\,dt -\frac{1}{h-x}\int_x^h f(x)\,dt\right|$$
    $$=\frac{1}{|h-x|}\left|\int_x^h(f(t)-f(x))\,dt\right|\leq\frac{1}{|h-x|}\left|\int_x^h|f(t)-f(x)|\,dt\right|.$$
    Essendo $f$ per ipotesi continua in $x$ vale che $|f(h)-f(x)|\leq\epsilon$ se $h\in[a,b]$ tale che $0<h-x\leq\delta$. Si ha quindi:
    $$\frac{1}{|h-x|}\left|\int_x^h|f(t)-f(x)|\,dt\right|\leq\frac{1}{|h-x|}\left|\int_x^h\epsilon\, dt\right|=\frac{1}{|h-x|}\epsilon|h-x|=\epsilon$$
\end{proof}
Dopo tutta questa fatica, almeno, il teorema seguente è gratuito.
\begin{theorem}[Teorema Fondamentale del Calcolo Integrale]
Sia $f:[a,b]\to\R$ una funzione integrabile secondo Riemann. Allora l'insieme degli integrali indefiniti di $f$ coincide con l'insieme delle primitive di $f$, ovvero: $$F(x)=c+\int_c^x f(t)\,dt.$$    
\end{theorem}
\begin{proof}
    Assumendo $c\in [a,b]$ si ha che $\int_{a}^{b}f = F + c$, che differisce da tutte le altre primitive per una costante. Per la continuità di $f$ e per il punto $2$ del teorema precedente si conclude.
\end{proof}

Nella determinazione esplicita della primitiva di una funzione è spesso opportuno o necessario utilizzare sostituzioni. Il seguente risultato ne regola la meccanica.

\begin{theorem}[Cambio di variabile]
Supponiamo che $f:[a,b]\to\R$ sia continua e\\ $\varphi:[c,d]\to[a,b]$ sia un'applicazione di classe $C^1$ con inversa di classe $C^1$. In queste ipotesi si ha
$$ \int_{a}^{b} f(x)\,dx \stackrel{x\mapsto\varphi(t)}{=}\int_{c}^{d}\varphi'(t) f(\varphi(t))\,dt = \int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)}\varphi'(t) f(\varphi(t))\,dt.$$
\end{theorem}
\begin{proof}
Cominciamo con l'osservare che la tesi è ovvia nel caso in cui $\varphi$ sia una mappa affine e invertibile, $\varphi(t)=mt+q$ con $m\neq 0$:

$$ \int_{a}^{b}f(x)\,dx = \int_{a-q}^{b-q}f(y+q)\,dy = m \int_{\frac{a-q}{m}}^{\frac{b-q}{m}}f(mt+q)\,dt = \int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)}\varphi'(t) f(\varphi(t))\,dt.$$
Per le proprietà algebriche dell'integrale la tesi resta vera nel caso in cui $\varphi:[c,d]\to[a,b]$ sia una funzione continua, lineare a tratti e strettamente monotona: in tal caso il grafico di $\varphi$ è una spezzata e $\varphi^{-1}$ è a sua volta una funzione continua, lineare a tratti e strettamente monotona. In questo caso $\varphi'(t)$ può non essere definita negli estremi dei lati della spezzata, ma poiché l'insieme dei punti di non derivabilità di $\varphi$ è un insieme discreto, il valore dell'integrale non ne risente. In virtù di quanto osservato, non è restrittivo assumere che nelle ipotesi del teorema che si abbia $[c,d]=[0,1]$. Dato $\varphi:[0,1]\to[a,b]$ diffeomorfismo di classe $C^1$, definiamo una sua approssimazione 

$$\varphi_n(t) = (1-\lambda)\varphi\left(\frac{\lfloor nt\rfloor}{n}\right)+\lambda \varphi\left(\frac{\lceil nt\rceil }{n}\right),\qquad \lambda= t-\frac{\lfloor nt\rfloor}{n} $$
che ha per grafico una spezzata ed è una funzione continua, lineare a tratti e strettamente monotona. Laddove è definita (ossia su tutto l'intervallo $[0,1]$, a meno dell'insieme discreto dato dai multipli interi di $\frac{1}{n}$) $\varphi_n'(t)$ dista da $\varphi'(t)$ al più il modulo di continuità di $\varphi'$ su intervalli di lunghezza $\frac{1}{n}$. Segue che $\varphi_n(t)$ converge uniformemente a $\varphi(t)$ su $[0,1]$ e si ha 

$$ \int_{a}^{b} f(x)\,dx = \int_{0}^{1}\varphi_n'(t)f(\varphi_n(t))\,dt = \lim_{n\to +\infty}\int_{0}^{1}\varphi_n'(t)f(\varphi_n(t))\,dt = \int_{0}^{1}\lim_{n\to +\infty}\varphi_n'(t)f(\varphi_n(t))\,dt = \int_{0}^{1}\varphi'(t)f(\varphi(t))\,dt.$$

\end{proof}

\textbf{Nota}: la dimostrazione appena esposta si estende abbastanza agilmente al caso di funzioni di più variabili. Inoltre l'idea di approssimare funzioni sufficientemente regolari con funzioni lineari a tratti può essere utilizzata (come fatto da Lebesgue) anche per dimostrare quanto segue:

\begin{theorem}[Di approssimazione di Weierstrass]. Rispetto alla norma del $\sup$ i polinomi sono densi in $C([a,b])$. In termini equivalenti, per qualunque funzione continua $f:[a,b]\to\R$ e per qualunque $\epsilon > 0$ esiste un polinomio $p(x)$ che realizza $|f(x)-p(x)|\leq\epsilon$ per ogni $x\in[a,b]$. 
\end{theorem}

Alla luce di questo risultato di \emph{densità}, in molti esercizi teorici sull'integrabilità secondo Riemann non è restrittivo supporre che la funzione integranda sia polinomiale.


\subsection{Sostituzioni da ricordare}
Nella pratica è piuttosto frequente imbattersi in integrali dove la funzione integranda coinvolge la radice quadrata di un polinomio di secondo grado. A seconda che tale polinomio abbia discriminante positivo o negativo è opportuno ricordare alcune sostituzioni notevoli.
\begin{itemize}
 \item (Discriminante negativo) La gestione del termine $\sqrt{x^2+1}$ è semplificata dalla sostituzione $x=\tan\theta$ o dalla sostituzione $x=\sinh t$. In un intorno dell'origine abbiamo infatti $\sqrt{\tan^2\theta+1}=\frac{1}{\cos\theta}$ mentre per il Teorema di Pitagora in versione iperbolica vale $\sqrt{\sinh^2 t+1}=\cosh t$, con $d\left(\tan\theta\right)=\frac{d\theta}{\cos^2\theta}$ e $d\left(\sinh t\right)=\cosh(t)\,dt$. 
 \item(Discriminante positivo) La gestione del termine $\sqrt{1-x^2}$ è semplificata dalla sostituzione $x=\sin\theta$ o dalla sostituzione $x=\cos\theta$. Per il Teorema di Pitagora $\sin^2\theta+\cos^2\theta=1$ e si ha $d\left(\sin\theta\right)=\cos(\theta)\,d\theta$, $d\left(\cos\theta\right)=-\sin(\theta)\,d\theta$. La gestione del termine $\sqrt{x^2-1}$ è semplificata dalla sostituzione $x=\cosh t$: per il Teorema di Pitagora in versione iperbolica $\sqrt{\cosh^2 t-1}=\sinh t$ e inoltre $d\left(\cosh t\right)=\sinh(t)\,dt$.
\end{itemize}
Attraverso sostituzioni della forma $x\mapsto \frac{1}{x}$ o $x\mapsto mx+q$ qualunque integrale della forma $\int_{a}^{b}\sqrt{p(x)}\,dx$ (o simili) è riconducibile ad uno dei casi esposti.
\begin{ex} Si determini la distanza media dal centro per un punto scelto a caso (rispetto ad una distribuzione di probabilità uniforme) all'interno di un quadrato unitario. 
\end{ex}
\textbf{Soluzione}. Quanto richiesto dal testo è 
$$ I = \int_{-1/2}^{1/2}\int_{-1/2}^{1/2}\sqrt{x^2+y^2}\,dx\,dy \stackrel{\text{simmetria}}{=} \frac{1}{2}\int_{0}^{1}\int_{0}^{1}\sqrt{u^2+v^2}\,du\,dv\stackrel{\text{simmetria}}{=} \int_{0}^{1}\int_{0}^{u}\sqrt{u^2+v^2}\,dv\,du.$$
L'integrale più interno nel membro destro è a sua volta esprimibile come 
$$\int_{0}^{u}\sqrt{u^2+v^2}\,dv = u^2\int_{0}^{1}\sqrt{1+z^2}\,dz $$
e il problema è così ricondotto alla determinazione di 
$$ \frac{1}{3}\int_{0}^{1}\sqrt{1+z^2}\,dz \stackrel{z\mapsto \sinh t}{=} \frac{1}{3}\int_{0}^{\arcsinh 1}\cosh^2(t)\,dt=\frac{1}{6}\int_{0}^{\arcsinh 1}\left(1+\cosh(2t)\right)\,dt=\frac{1}{6}\left[t+\sinh(t)\cosh(t)\right]_{0}^{\arcsinh 1}$$
che equivale a 
$$ \frac{1}{6}\left(\arcsinh{1}+\cosh(\arcsinh{1})\right) = \frac{\sqrt{2}+\arcsinh{1}}{6}={\color{blue}\frac{\sqrt{2}+\log(1+\sqrt{2})}{6}}\approx 0.3826. $$
Un'altra sostituzione che è bene tenere a mente, specie nel caso in cui la funzione integranda sia strettamente correlata alle funzioni trigonometriche elementari, è la sostituzione $\theta=\arctan t$. La analizziamo nel prossimo esempio.
\begin{ex} Si determini esplicitamente il valore dell'integrale 
$$ I = \int_{0}^{\pi}\frac{d\theta}{5+\cos\theta}.$$
\end{ex}
\textbf{Soluzione}. Per traslazione della variabile di integrazione e simmetria si ha 
$$ I = \int_{-\pi/2}^{\pi/2}\frac{d\theta}{5-\sin\theta} = \int_{0}^{\pi/2}\left(\frac{1}{5-\sin\theta}+\frac{1}{5+\sin\theta}\right)\,d\theta = 10\int_{0}^{\pi/2}\frac{d\theta}{25-\sin^2\theta}=10\int_{0}^{\pi/2}\frac{d\theta}{25-\cos^2\theta}. $$
Invocando la sostituzione $\theta=\arctan t$, per cui $d\theta=\frac{dt}{1+t^2}$ e $\cos^2(\arctan t)=\frac{1}{1+t^2}$, ci riconduciamo immediatamente a
$$ I = 10\int_{0}^{+\infty}\frac{dt}{25(1+t^2)-1}=\frac{2}{5}\int_{0}^{+\infty}\frac{dt}{t^2+\frac{24}{25}}\stackrel{t=\sqrt{\frac{24}{25}}u}{=}\frac{2}{5}\sqrt{\frac{24}{25}}\int_{0}^{+\infty}\frac{du}{\frac{24}{25}(u^2+1)}=\frac{2}{5}\sqrt{\frac{25}{24}}\frac{\pi}{2} = {\color{blue}\frac{\pi}{2\sqrt{6}}}. $$


\subsection{Integrazione per parti}
\begin{theorem}[Integrazione per parti]
Nell'ipotesi che le funzioni integrande in ambo i membri siano continue su $[a,b]$ si ha che 
$$ \int_{a}^{b} f(x)g(x) = \left[ F(x)g(x)\right]_{a}^{b}-\int_{a}^{b} F(x)g'(x)\,dx $$
dove $F(x)$ denota \emph{una} primitiva di $f(x)$.
\end{theorem}
\begin{proof} È sufficiente considerare, per $c\in[a,b]$, le funzioni 
$$ \varphi(c) = \int_{a}^{c}f(x)g(x)\,dx,\qquad \psi(c)=[F(x)g(x)]_{a}^{c}-\int_{a}^{c}F(x)g'(x)\,dx.$$
Per il Teorema fondamentale del calcolo e la regola di Leibniz $\varphi$ e $\psi$ sono funzioni derivabili su $[a,b]$ con la stessa derivata. Poiché $\varphi(a)=\psi(a)=0$, per ogni $c\in[a,b]$ si ha $\varphi(c)=\psi(c)$. 
\end{proof}

\begin{ex} Si dimostri che per ogni $t\in\R^+$ la quantità 
$$ \int_{0}^{t}\frac{\sin x}{x}\,dx $$
è positiva. 
\end{ex}
\textbf{Soluzione}. Scegliendo $1-\cos x = 2\sin^2\frac{x}{2}$ come antiderivata del seno ed applicando integrazione per parti si ha che 
$$ \int_{0}^{t}\frac{\sin x}{x}\,dx = \frac{1-\cos t}{t}+2\int_{0}^{t}\left(\frac{\sin\frac{x}{2}}{x}\right)^2\,dx $$
dove il primo addendo nel membro destro è sicuramente non negativo e il secondo addendo è sicuramente positivo.

\subsection{Un caso interessante}
Seguono gli integrali delle potenze del coseno, \emph{talvolta} utili:
$$\int_{0}^{\frac{\pi}{2}}(\cos \theta)^{2n}d\theta=\frac{\pi}{2\cdot4^n}\binom{2n}{n}$$
$$\int_{0}^{\frac{\pi}{2}}(\cos \theta)^{2n+1}d\theta= \frac{4^n}{(2n+1)\binom{2n}{n}}$$
inaspettatamente questi due integrali sono strettamente legati ai binomiali centrali e sono appunto una loro rappresentazione integrale.
\subsection{Decomposizione in fratti semplici}
L'integrazione di funzioni razionali (quozienti di polinomi) è puramente algoritmica, in quanto sempre riconducibile all'integrazione di funzioni della forma $\frac{1}{(x-\alpha)^n}$ (eventualmente con $\alpha\in\C$). Analizziamo la situazione attraverso un paio di esempi concreti.
\begin{ex} Si determini esplicitamente
$$ \int_{0}^{1}\frac{x^6}{(x+1)^2(x+2)(x^2+1)}\,dx $$ 
\end{ex}
\textbf{Soluzione}. Un primo step è ricondursi all'integrazione di $\frac{p(x)}{q(x)}$ dove $p(x),q(x)$ non hanno fattori comuni e $\deg p < \deg q$. Nel nostro caso il grado di $x^6$ supera di $1$ il grado di $(x+1)^2(x+2)(x^2+1)$, dunque 
$$ \frac{x^6}{(x+1)^2(x+2)(x^2+1)} = (Ax+B)+\frac{p(x)}{(x+1)^2(x+2)(x^2+1)}$$
dove $(Ax+B)$ è il quoziente e $p(x)$ è il resto della divisione tra $x^6$ e $(x+1)^2(x+2)(x^2+1)$. I coefficienti $A$ e $B$ possono essere determinati eseguendo la divisione in colonna, ricorrendo al Teorema cinese del resto o anche attraverso stratagemmi \emph{ad hoc}. Nel nostro caso possiamo ad esempio osservare che $x^6$ differisce di $1$ da un multiplo di $x^2+1$, via $x^6+1=(x^2+1)(x^4-x^2+1)$. Rappresentando $x^6$ come $(x^6+1)-1$ abbiamo pertanto 
$$ \frac{x^6}{(x+1)^2(x+2)(x^2+1)} = \frac{x^4-x^2+1}{(x+1)^2(x+2)}-\frac{1}{(x+1)^2(x+2)(x^2+1)}$$
e possiamo nuovamente osservare che $x^4-x^2$ è un multiplo di $x+1$, via $x^4-x^2=x^2(x-1)(x+1)$. Ciò conduce a 
$$ \frac{x^6}{(x+1)^2(x+2)(x^2+1)} = \frac{x^2(x-1)}{(x+1)(x+2)}+\frac{1}{(x+1)^2(x+2)}-\frac{1}{(x+1)^2(x+2)(x^2+1)}$$
da cui facilmente segue 
$$ \frac{x^6}{(x+1)^2(x+2)(x^2+1)} = (x-4)-\frac{2}{x+1}+\frac{12}{x+2}+\frac{1}{(x+1)^2(x+2)}-\frac{1}{(x+1)^2(x+2)(x^2+1)}.$$
L'integrazione su $[0,1]$ dei primi tre addendi del membro destro è immediata.\\ Il problema è così ricondotto alla determinazione di 
$$I_1=\int_{0}^{1}\frac{dx}{(x+1)^2(x+2)},\qquad I_2=\int_{0}^{1}\frac{dx}{(x+1)^2(x+2)(x^2+1)}.$$
Rammentiamo che $$\frac{1}{(n+a)(n+b)}=\frac{1}{b-a}\left(\frac{1}{n+a}-\frac{1}{n+b}\right),$$
da cui segue 
\begin{eqnarray*} \frac{1}{(x+1)^2(x+2)}&=&\frac{1}{x+1}\left(\frac{1}{x+1}-\frac{1}{x+2}\right)=\frac{1}{(x+1)^2}-\frac{1}{x+1}+\frac{1}{x+2}\end{eqnarray*}
che prova immediatamente $I_1=\frac{1}{2}-\log\left(\frac{4}{3}\right)$. Infine 
$$ \frac{1}{(x+1)^2(x+2)(x^2+1)}=\frac{C}{x+1}+\frac{D}{x+2}+\frac{E}{x+i}+\frac{F}{x-i}+\frac{G}{(x+1)^2}.$$
Con sufficiente pazienza i coefficienti $C,D,E,F,G$ possono essere determinati risolvendo un sistema lineare in $5$ equazioni e $5$ incognite. Ma anche se, come in un noto proverbio africano, \emph{Dio è lento e voi avete fretta}, le strategie abbondano. Il coefficiente $G$ può essere determinato moltiplicando ambo i membri per $(x+1)^2$ e poi considerando il limite per $x\to -1$: segue che $G=\frac{1}{2}$. Analogamente, moltiplicando ambo i membri per $(x+2)$ e considerando il limite per $x\to -2$ abbiamo $D=\frac{1}{5}$. I coefficienti $E,F$ sono necessariamente coniugati, per cui da $F=\lim_{x\to i}(x-i)f(x)=-\frac{1}{10}+\frac{i}{20}$ discende $E=-\frac{1}{10}-\frac{i}{20}$. Infine, poiché moltiplicando ambo i membri per $x$ e considerando il limite per $x\to +\infty$ si ottiene $0$, $C+D+E+F=0$, da cui $C=0$. In conclusione 
$$ \frac{1}{(x+1)^2(x+2)(x^2+1)} = \frac{1}{5}\cdot\frac{1}{x+2}-\frac{1}{10}\cdot\frac{1}{x^2+1}-\frac{1}{5}\cdot\frac{x}{x^2+1}+\frac{1}{2}\cdot\frac{1}{(x+1)^2} $$
che comporta $I_2=\frac{1}{40}\left(10-\pi+8\log 3-12\log 2\right)$ e 
$$ \int_{0}^{1}\frac{x^6\,dx}{(x+1)^2(x+2)(x^2+1)}=\frac{1}{40}\left(\pi-130-628\log 2+512\log 3\right).\qed $$
\begin{ex} Si determini esplicitamente il valore dell'integrale 
$$ I=\int_{0}^{1}\frac{dx}{(x^2+1)(x^2+3)}. $$
\end{ex}
\textbf{Soluzione}. Analogamente a prima potremmo determinare i \emph{residui} di $f(x)=\frac{1}{(x^2+1)(x^2+3)}$ in corrispondenza di $x=\pm i$ e $x=\pm i\sqrt{3}$, ma possiamo anche solo constatare che 
$$ \frac{1}{(x^2+1)(x^2+3)} = \frac{1}{2}\left(\frac{1}{x^2+1}-\frac{1}{x^2+3}\right) $$
da cui, immediatamente:
$$ I = \frac{1}{2}\int_{0}^{1}\frac{dx}{x^2+1}-\frac{1}{2}\int_{0}^{1}\frac{dx}{x^2+3}=\frac{\pi}{8}-\frac{1}{2\sqrt{3}}\int_{0}^{1/\sqrt{3}}\frac{dz}{z^2+1} = \frac{\pi}{8}-\frac{\pi}{12\sqrt{3}}.\qed$$

La decomposizione in fratti semplici è utile non solo nel contesto dell'integrazione di funzioni razionali, ma anche nell'approssimazione numerica (o nel calcolo esplicito) di serie.

\begin{ex} Si determini esplicitamente 
$$ S=\sum_{n\geq 1}\frac{1}{n(n+1)(n+4)}.$$
\end{ex}
\textbf{Soluzione}. La decomposizione in fratti semplici di $f(x)=\frac{1}{x(x+1)(x+4)}$ può essere ottenuta come segue:
$$ f(x)=\frac{1}{x+4}\left(\frac{1}{x}-\frac{1}{x+1}\right) = \frac{1}{4}\left(\frac{1}{x}-\frac{1}{x+4}\right)-\frac{1}{3}\left(\frac{1}{x+1}-\frac{1}{x+4}\right) $$
e ciò comporta immediatamente 
$$ S = \frac{1}{4}\sum_{n\geq 1}\left(\frac{1}{n}-\frac{1}{n+4}\right)-\frac{1}{3}\sum_{n\geq 1}\left(\frac{1}{n+1}-\frac{1}{n+4}\right) = \frac{H_4}{4}-\frac{H_4-1}{3} = \frac{1}{3}-\frac{H_4}{12} = \frac{23}{144}.\qed$$
(Dove $H_n$ rappresenta l'\emph{n-}esimo numero armonico)\\

\textbf{Soluzione alternativa}. A partire da 
$$ \sum_{n\geq 1}\frac{x^{n+3}}{n(n+1)} = x^3+x^2(1-x)\log(1-x), $$
valida per ogni $x\in(-1,1)$, integrando ambo i membri su $(0,1)$ otteniamo 
$$ S = \frac{1}{4}+\int_{0}^{1}x^2(1-x)\log(1-x)\,dx = \frac{1}{4}+\int_{0}^{1}(1-z)^2 z \log(z)\,dz  $$
e per integrazione per parti 
$$ S = \frac{1}{4}+\left[\left(\frac{z^2}{2}-\frac{2z^3}{3}+\frac{z^4}{4}\right)\log z\right]_{0}^{1}-\int_{0}^{1}\left(\frac{z}{2}-\frac{2z^2}{3}+\frac{z^3}{4}\right)\,dz=\frac{1}{4}-\frac{1}{4}+\frac{2}{9}-\frac{1}{16} = \frac{23}{144}.\qed$$

\subsection{Integrale di Riemann improprio}
L'integrale di Riemann in senso proprio è definito a partire da funzioni limitate su domini limitati. Nella pratica è tuttavia frequente confrontarsi con funzioni o domini illimitati, ed è bene aver chiaro come gestire tali situazioni, in particolare conoscere quali proprietà algebriche dell'integrale proprio restano valide e quali cessano di essere valide.\\

\textbf{Caso \#1: funzione limitata su dominio illimitato}, $\int_{a}^{+\infty}f(x)\,dx, \int_{-\infty}^{b}f(x)\,dx, \int_{-\infty}^{+\infty}f(x)\,dx.$\\
È il caso di più semplice gestione: considerata una \emph{esaustione in compatti} $K_1\subset K_2 \subset K_3 \subset \ldots \to D$ del dominio di integrazione, definiamo $\int_D f(x)\,dx$ come $\lim_{n\to +\infty}\int_{K_n}f(x)\,dx$, posto ovviamente che questo esista. Nel caso in cui il dominio di integrazione sia illimitato soltanto da un lato, questo è equivalente a porre
$$ \int_{a}^{+\infty}f(x)\,dx = \lim_{b\to +\infty}\int_{a}^{b}f(x)\,dx,\qquad \int_{-\infty}^{b}f(x)\,dx = \lim_{a\to -\infty}\int_{a}^{b}f(x)\,dx. $$
Nel caso in cui il dominio di integrazione sia illimitato da ambo i lati, ossia coincida con $\R$, ciò \textbf{non è} equivalente a porre $\int_{\R}f(x)\,dx = \lim_{M\to +\infty}\int_{-M}^{M}f(x)\,dx$, che fornisce solo l'integrale in senso simmetrico, o nel senso del \emph{valore principale} di Cauchy. Risulta 
$$ \int_{\R}f(x)\,dx = \int_{-\infty}^{0}f(x)\,dx + \int_{0}^{+\infty} f(x)\,dx $$
solo a patto che entrambi gli addendi nel membro destro siano finiti. Per intenderci: per quanto $\sin x$ sia una funzione limitata e dispari, l'integrale di Riemann improprio $\int_{\R}\sin(x)\,dx$ \textbf{non esiste}.\\

\textbf{Caso \#2: funzione illimitata su dominio limitato}, $\int_{0}^{1}\frac{dx}{\sqrt{x}}$.\\
In questo caso, anziché ``approssimare dall'interno'' il dominio di integrazione, approssimiamo dall'interno il sottografico dell'integranda. Per ogni $M>0$ poniamo
$$ f_M(x) = \left\{\begin{array}{rcl}-M &\text{se}& f(x)\leq -M \\ f(x) & \text{se} & |f(x)|\leq M \\ M &\text{se}& f(x)\geq M\end{array}\right. $$
e definiamo l'integrale improprio $\int_{a}^{b}f(x)\,dx$ come 
$$ \lim_{M\to +\infty} \int_{a}^{b} f_M(x)\,dx, $$
posto ovviamente che tale limite esista.\\

\textbf{Caso \#3: funzione illimitata su dominio illimitato}, $\int_{0}^{+\infty}\frac{dx}{e^x\sqrt{x}}$.\\
In questo caso combiniamo le precedenti estensioni: è un esercizio tedioso ma non difficile verificare che l'esito è il medesimo indipendentemente dalla scelta di approssimare per primo il dominio di integrazione o la funzione integranda.\\

Un ulteriore esercizio tedioso è verificare che restano valide le proprietà di linearità e monotonia e le meccaniche di integrazione per parti o mediante sostituzioni, ma l'integrale di Riemann in senso improprio non coincide necessariamente con il limite delle somme di Riemann, come evidenziato nell'esercizio \textbf{J33}.

\begin{ex}[Integrale di Fresnel] Si dimostri che la funzione $f(x)=\sin(x^2)$ è impropriamente Riemann-integrabile su $\R^+$ e si determini esplicitamente il valore dell'integrale. 
\end{ex}
\textbf{Soluzione}. La sostituzione $x=\sqrt{t}$ conduce a 
$$ I=\int_{0}^{+\infty}\sin(x^2)\,dx = \frac{1}{2}\int_{0}^{+\infty}\frac{\sin t}{\sqrt{t}}\,dt $$ 
dove l'ultimo integrale esiste finito per il criterio di Abel-Dirichlet. Esprimendo $\frac{1}{\sqrt{t}}$ come $\int_{0}^{+\infty}\frac{1}{\sqrt{\pi x}} e^{-tx}\,dx$ ed invocando il Teorema di Fubini abbiamo inoltre che 
$$ I = \frac{1}{2\sqrt{\pi}}\int_{0}^{+\infty}\frac{1}{\sqrt{x}}\int_{0}^{+\infty}\sin(t)e^{-tx}\,dt \,dx = \frac{1}{2\sqrt{\pi}}\int_{0}^{+\infty}\frac{dx}{(x^2+1)\sqrt{x}}\stackrel{x\mapsto z^2}{=}\frac{1}{\sqrt{\pi}}\int_{0}^{+\infty}\frac{dz}{z^4+1}={\color{blue}\sqrt{\frac{\pi}{8}}}. $$

\begin{ex} Si determini per quali valori dei parametri $\alpha,\beta > 0$ si ha che l'integrale 
$$ I(\alpha,\beta) = \int_{1}^{+\infty}\frac{dx}{x^\alpha \ln^\beta x}$$
esiste finito. 
\end{ex}
\textbf{Soluzione}. In virtù della sostituzione $x\mapsto\exp t$ si ha che 
$$ I(\alpha,\beta) = \int_{0}^{+\infty}t^{-\beta} e^{-(\alpha-1)t}\,dt $$
dove l'ultima funzione integranda si comporta come $t^{-\beta}$ in un intorno destro dell'origine. Su tale insieme la Riemann-integrabilità è equivalente alla condizione $\beta < 1$, e per avere Riemann-integrabilità anche in un intorno sinistro di $+\infty$ è necessario e sufficiente che si abbia $\alpha>1$. In queste ipotesi 
$$ I(\alpha,\beta) = \frac{1}{(\alpha-1)^{\beta+1}}\int_{0}^{+\infty} z^{-\beta} e^{-z}\,dz = \frac{\Gamma(1-\beta)}{(\alpha-1)^{\beta+1}}.$$

\begin{ex}[Fallacia dei Fisici] Si dimostri che una funzione positiva e impropriamente Riemann-integrabile su $\R^+$ non ha necessariamente limite zero all'infinito. 
\end{ex}
\emph{Sketch di controesempio}: $f(x) = \sum_{n\geq 1} n\exp\left(-n^6(x-n)^2\right)$ è positiva e impropriamente Riemann-integrabile su $\R^+$, ma $f(n)\geq n$ per ogni $n\in\N^+$ mentre $\lim_{n\to +\infty} f\left(n+\frac{1}{2}\right)=0$.

\section{Successioni per ricorrenza}
Lo studio di successioni in cui il termine generale $a_n$ dipende in qualche modo dai precedenti è di fondamentale importanza non solo dal punto di vista strettamente teorico, ma anche da quello fisico-ingegneristico, visto che è piuttosto raro essere in grado di ricostruire esplicitamente la soluzione di un'equazione differenziale, ma si hanno migliori chances di successo nella trattazione (eventualmente numerica) dei corrispondenti problemi \emph{linearizzati} o altrimenti \emph{discretizzati}. Il primo caso che ci apprestiamo a trattare è quello delle successioni per ricorrenza \emph{lineari} ed \emph{autonome}, dove il termine generale $a_n$ è una fissata combinazione lineare dei precedenti $k$ termini.

\subsection{Successione di Fibonacci e similia}
Un problema prototipico è quello di determinare una formula esplicita (\emph{formula di Binet}) per la successione (\emph{di Fibonacci}) $\{F_n\}_{n\geq 0}$ definita dalle \emph{condizioni iniziali} $F_0=0, F_1=1$ e dalla (legge di) \emph{ricorrenza} $F_{n+2}=F_{n+1}+F_n$, valida per ogni $n\in\N$. Illustriamo diversi approcci equivalenti.\\

\textbf{Approccio \#1, \emph{rasoio di Occam}}. L'insieme delle successioni $\{a_n\}_{n\geq 0}$ che soddisfano la ricorrenza $a_{n+2}=a_{n+1}+a_n$ è uno \emph{spazio vettoriale}. In uno slancio di ottimismo, cerchiamo elementi ragionevolmente \emph{semplici} di questo spazio vettoriale, ossia \emph{successioni di potenze} $a_n=\alpha^n$ che rispettano la ricorrenza.
Imporre che si abbia $a_{n+2}=a_{n+1}+a_n$ è equivalente ad imporre che la quantità $\alpha$ soddisfi $\alpha^2=\alpha+1$, ossia risulti radice del \emph{polinomio caratteristico} $p(x)=x^2-x-1$. Per quanto noto dall'antichità più remota sulle proporzioni del pentagono regolare, le radici del precedente polinomio sono il \emph{rapporto aureo} $\varphi=\frac{1+\sqrt{5}}{2}$ e il suo coniugato algebrico $\overline{\varphi}=\frac{1-\sqrt{5}}{2}$. In virtù della struttura di spazio vettoriale, \emph{tutte} le successioni della forma $a_n = A\varphi^n + B\overline{\varphi}^n$ soddisfano la ricorrenza voluta. Nell'ultima espressione abbiamo due gradi di libertà nella scelta dei coefficienti $A,B$, ed auspicabilmente un'opportuna scelta della coppia $(A,B)$ è tale da garantire simultaneamente sia $a_0=F_0=0$ che $a_1=F_1=1$, ossia 
$$ \left\{\begin{array}{rcl}\varphi^0 A + \overline{\varphi}^0 B &=& 0 \\ \varphi^1 A + \overline{\varphi}^1 B &=& 1.\end{array}\right. $$
In virtù della regola di Cramer il sistema ha effettivamente soluzione unica, poiché 
$$ \det \begin{pmatrix} 1 & 1 \\ \varphi & \overline{\varphi} \end{pmatrix}=\overline{\varphi}-\varphi \neq 0, $$
caso particolare del fatto che tutte le matrici di Vandermonde definite a partire da elementi distinti sono invertibili.\\
A conti fatti abbiamo $A=-B=\frac{1}{\sqrt{5}}$, dunque la successione $a_n=\frac{\varphi^n-\overline{\varphi}^n}{\sqrt{5}}$ rispetta sia la legge di ricorrenza che le condizioni iniziali volute. Dal principio di induzione segue pertanto $a_n=F_n$, ossia 
$$ F_n = \frac{\varphi^n-\overline{\varphi}^n}{\sqrt{5}}. $$

\textbf{Approccio \#2, \emph{Jordan}}. Possiamo liberamente definire il vettore $v_n=\begin{pmatrix} F_{n+1} \\ F_{n}\end{pmatrix}$ ed esprimere la ricorrenza nella forma $v_{n+1} = M v_n$, dove $M=\begin{pmatrix} 1 & 1 \\ 1 & 0\end{pmatrix}$. Dal principio di induzione segue $v_n = M^n v_0$, dunque il problema di esplicitare il termine generale della successione si rivela equivalente a quello di esplicitare l'$n$-esima potenza della matrice $M$. Dal Teorema di Jordan si ha $M = J^{-1}\begin{pmatrix}\varphi & 0 \\ 0 & \overline{\varphi}\end{pmatrix} J$ per un'opportuna matrice invertibile $J$, da cui $M^n = J^{-1}\begin{pmatrix}\varphi^n & 0 \\ 0 & \overline{\varphi}^n\end{pmatrix} J$. Segue che il termine generale della successione è una fissata combinazione lineare di $\varphi^n$ e $\overline{\varphi}^n$, dove i coefficienti di questa combinazione lineare possono essere facilmente calcolati per interpolazione, o (meno efficientemente) esplicitando la matrice $J$.\\

\textbf{Approccio \#3, \emph{OGF}}. Questo è un approccio estremamente potente, ma decisamente indiretto: associamo alla successione una funzione regolare, dalla ricorrenza deduciamo proprietà algebriche della funzione, dalle ultime ricostruiamo il comportamento esplicito (o anche solo asintotico) della successione. È immediato realizzare che la successione di Fibonacci è crescente, ma non troppo rapidamente, in quanto si ha $a_{n+1}\leq 2 a_n$ per ogni $n\geq 1$, da cui $F_n \leq 2^{n-1}$.\\ In particolare, posto 
$$ f(x) = \sum_{n\geq 0} F_n x^n  = x + \sum_{n\geq 2}F_n x^n$$
si ha che $f(x)$ è una funzione analitica in un intorno circolare dell'origine di raggio almeno $\frac{1}{2}$.\\ Su questo insieme abbiamo 

$$ x^2 f(x) = \sum_{n\geq 0} F_n x^{n+2} = \sum_{n\geq 2} F_{n-2} x^n,\qquad x f(x) = \sum_{n\geq 0} F_n x^{n+1} = \sum_{n\geq 2} F_{n-1} x^n $$
e dalla relazione $F_{n}=F_{n-1}+F_{n-2}$, valida per ogni $n\geq 2$, deduciamo che 
$$ f(x) = x + x f(x) + x^2 f(x)\quad\Longrightarrow\quad f(x) = \frac{x}{1-x-x^2}. $$
Operando una decomposizione in fratti semplici dell'ultima funzione razionale, mediante serie geometriche otteniamo che 
$$ f(x) = \frac{1}{\sqrt{5}}\left(\frac{1}{1-\varphi x}-\frac{1}{1-\overline{\varphi}x}\right) = \frac{1}{\sqrt{5}}\sum_{n\geq 0}\left(\varphi^n-\overline{\varphi}^n\right) x^n. $$
Dal principio di identità delle funzioni analitiche (diretta estensione del principio di identità dei polinomi: se due funzioni analitiche coincidono in un intorno dell'origine hanno le medesime derivate in zero e viceversa) segue la formula di Binet. $f(x)$ è detta \emph{funzione generatrice ordinaria (OGF)} della successione $\{F_n\}_{n\geq 0}$ e l'effettivo raggio di convergenza nell'origine è dato dal reciproco del massimo modulo delle radici del polinomio caratteristico (Teorema di Frobenius), nel nostro caso $\frac{1}{\varphi}=\varphi-1\approx 0.618$.\\

Gli ultimi due approcci sono immediatamente generalizzabili al caso in cui una successione per ricorrenza lineare ed autonoma abbia ordine $k\geq 2$. Il primo approccio è praticabile solo se le radici del polinomio caratteristico sono tutte distinte: altrimenti si manifestano problemi di \emph{risonanza}, legati alle potenze di una matrice non diagonalizzabile, o ai \emph{poli} di funzioni razionali con ordine maggiore di $1$. Vediamone un esempio.

\begin{ex} La successione $\{a_n\}_{n\geq 0}$ è definita dalla condizioni iniziali $a_0=1, a_1=2, a_2=5$ e dalla ricorrenza $a_{n+3}=3 a_{n+2}-4 a_n$, valida per ogni $n\in\N$. Si determini un'espressione esplicita del termine generale. 
\end{ex}

\textbf{Soluzione}. Il polinomio caratteristico della ricorrenza è $p(x)=x^3-3x^2+4=(x-2)^2(x+1)$, dunque le successioni $b_n=2^n$ e $c_n=(-1)^n$ soddisfano la ricorrenza. Due elementi non sono sufficienti a generare uno spazio vettoriale di dimensione $3$, ma il ``terzo escluso'' può essere facilmente determinato considerando che la matrice compagna del polinomio $p(x)$ è simile a 
$$\begin{pmatrix} 2 & 1 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & -1\end{pmatrix}, $$
dunque lo spazio vettoriale è generato da $2^n, n 2^n$ e $(-1)^n$ e $a_n = A 2^n + B n 2^n + C(-1)^n$ con 
$$\left\{\begin{array}{rcl}A+C&=& 1 \\ 2A+2B-C&=& 2 \\ 4A+8B+C&=&5\end{array}\right. $$
che conduce a $A=\frac{8}{9}$, $B=\frac{1}{6}$, $C=\frac{1}{9}$.\qed

\begin{ex}[Tribonacci] In funzione di $n\in\N^+$ si determini il numero $T_n$ di stringhe binarie di lunghezza $n$ all'interno delle quali non sono mai presenti tre caratteri $1$ consecutivi. Si determini infine una buona approssimazione numerica del limite $\lim_{n\to +\infty}\sqrt[n]{T_n}$.
 
\end{ex}



\newpage

\section{Equazioni Differenziali}
Per il Teorema Fondamentale del Calcolo Integrale (T.F.C.I.) possiamo risalire alla funzione $u(t)$ se se ne conosce la derivata, infatti l'equazione $$u'(t)=f(t)$$ ha soluzione $$u(t)=\int_{a}^{t}f(s)ds$$
In generale, data una funzione di due variabili $f(t,u)$ possiamo porci il problema di trovare una funzione $u(t)$ che verifichi l'equazione differenziale:$$u'(t)=f(t,u(t))$$
l'equazione di sopra si dice del primo ordine perché coinvolge solo la derivata prima della funzione $u(t)$.
\subsection{ED a Variabili Separabili}
si chiamano così le equazioni del tipo: $$u'(t)=a(t)f(u(t))$$ per risolvere queste equazioni basta dividere entrambi i membri per $f(u)$ ed integrare: $$\int\frac{u'(t)}{f(u(t))}dt=\int a(t)dt$$
\subsection{ED Lineari del Primo ordine}
sono le equazioni del tipo: $$u'(t)=a(t)u(t)+b(t)$$
con $a(t)$ e $b(t)$ funzioni date.\\
Indichiamo con $A(t)=\int a(t)dt$ si ha che, se $b(t)=0$ $$log(u(t))=A(t)+p$$ con p costante arbitraria.\\
quindi $$u(t)= ce^{A(t)}$$
con $c=e^p$ costante arbitraria
\subsection{Esempi}
\subsubsection{Decadimento Radioattivo}
è il processo secondo il quale un sostanza radioattiva decade, trasformandosi in un'altra sostanza più leggera a causa della perdita di neutroni.\\
Il carattere fondamentale di questa reazione è che non si può predire quando avverà, ma si può dare una probabilità a questo evento. Questa probabilità è $pdt$ proporzionale all'intervallo di tempo $dt$, indicando con $n(t)$ il numero di neutroni presenti al tempo $t$ tra $t$ e $t*dt$ se ne disintegreranno $n(t)pdt$. Il numero complessivo di neutroni diminuirà, ottendo quindi:$$\frac{n(p+dt)-n(t)}{dt}=-pn(t)$$
facendo tendere $dt$ a $0$ otteniamo: $$n'(t)=n_0e^{-pt}$$\\
il numero di neutroni decresce quindi esponenzialmente e si denota con $\tau=\frac{log2}{p}$ il tempo di dimezzamento
\subsubsection{Oscillatore Armonico}
Con questo termine si indica un sistema costituito da un punto materiale di massa $m$ che si muove su una rett, soggette ad una forza di richiamo, proporzionale alla distanza dal punto $0$. \\
indicando con $u(t)$ la posizione del corpo al tempo $t$, la forza totale che agisce sul corpo sarà: $f=-ku-hu'$ e l'equazione del moto $f=ma$ si scrive: $$mu''(t)+hu'(t)+ku(t)=0$$\\
ponendo $\omega_0^2=\frac{k}{m}$ e $a=\frac{h}{2m}$.\\
$$u''(t)+2au'(t)+\omega_0^2u=0$$ \\
se cerchiamo soluzioni della forma $e^{\lambda t}$ otteniamo l'equazione $\lambda^2 + \omega_0^2=0$ che ha radici complesse $i\omega_0t$ e $-i\omega_0t$ che corrispondono alle soluzioni : $e^{i\omega_0t}$ e $e^{-i\omega_0t}$\\
dove  $e^{i\omega_0t}$ e $e^{-i\omega_0t}$ stanno per $cos\omega_0t+isen\omega_0t$ e $cos\omega_0t-isen\omega_0t$ \\
La soluzione generale della differenziale è quindi: $$u(t)=c_1sen\omega_0t+c_2cos\omega_0t$$\\
\subsection{Metodi di Risoluzione di ED non omogenee}
In generale consideriamo $\Tilde{x_{OM}}$ la soluzione dell'omogenea. Allora la soluzione generale sarà del tipo: $$\Tilde{x}= \Tilde{x_{OM}}+ \Tilde{x_1}$$\\
dove $\Tilde{x_1}$ è la soluzione generale dell'equazione differenziale.\\
Per trovarla è utile il \textbf{Metodo di Somiglianza} che consiste nel cercare le soluzioni generali a partire dal termine noto dell'equazione.\\
Se per esempio stiamo cercando di risolvere l'equazione:
$$u'(t)+2u(t)=t^2$$
Cerchiamo prima la base delle soluzione dell'omogenea:$$u'(t)+2u(t)=0\Rightarrow \lambda +2 =0 \Rightarrow \lambda=-2 \Rightarrow \Tilde{x_{OM}}=e^{-2t}$$
Per trovare la soluzione generale dell'equazione ora guardiamo il termine noto $t^2$, la speranza è quella di trovare come soluzioni una funzione della stessa forma di $t^2$ in questo caso, cerchiamola tra i polinomi.\\
Supponiamo quindi che $\Tilde{x_1}=at^3+bt^2+ct+d$ sia una soluzione, allora derivando otteniamo: $\Tilde{x_1}'=3at^2+2bt+c$, utilizzando la relazione data dalla differenziale sappiamo che:
$$3at^2+2bt+c + 2at^3+2bt^2+2ct+2d=t^2\Leftrightarrow 3a+2a=1\Rightarrow a=\frac{1}{5}$$
La soluzione totale è quindi $\Tilde{x}= c_1e^{-2t}+c_2\frac{1}{5}t^3$\\
Un altro metodo per risolvere alcune differenziali (più difficili delle precedenti) è quello di studiare il comportamento di $u'(t)$ con la sua inversa $v'(t)$, se esiste, o analogamente, se è ben definito, col suo reciproco.
\section{Esercizi e simulazioni di pre-test}
Gli esercizi di seguito riportati con una stella ($\star$) hanno difficoltà paragonabile a quelli del compito, quelli riportati con due stelle ($\star\star$) sono più impegnativi. Quelli con tre stelle ($\star\star\star$) sono estremamente impegnativi, si raccomanda cautela.\\


 \textbf{J1}($\star$). Si dimostri che 
\begin{equation*}
 \sum_{n\geq 1}\frac{1}{5^n}\binom{2n}{n}   
\end{equation*}
è una serie convergente.


\textbf{J2}($\star$). Si determini il valore del seguente integrale: 
\begin{equation*}
 \int_{0}^{+\infty}\frac{\ln t}{1+t^2}\,dt   
\end{equation*}


\textbf{J3}($\star$). Si dimostri che per qualunque $k\in\mathbb{N}$ la serie

 \begin{equation*}
    a_k = \sum_{n\geq 1}\frac{n^k}{2^n} 
 \end{equation*}
converge ad un numero naturale.  \par
$(\star\star\star)$ Si provi che 
\begin{equation*}
    a_k\sim \frac{k!}{\ln(2)^{k+1}}
\end{equation*} quando $k\to +\infty$.


\textbf{J4}$(\star)$. Si dimostri che il seguente limite esiste ed è finito:

$$ \lim_{N\to +\infty}\left(-2\sqrt{N}+\sum_{n=1}^{N}\frac{1}{\sqrt{n}}\right).$$

\textbf{J5}$(\star)$. Si approssimi il valore della seguente serie convergente con un errore non superiore al millesimo:
$$ \sum_{n\geq 1}\left(1-\cos\frac{1}{n}\right)$$

\textbf{J6}$(\star)$. Per ogni $n\in\mathbb{N}^+$ il polinomio $P_n(x)$ è definito come segue:
$$ P_n(x) = \frac{1}{n!}\cdot\frac{d^n}{dx^n}\left(x(x-1)\right)^n.$$
Si dimostri che $P_n(x)$ ha $n$ radici reali distinte nell'intervallo $(0,1)$ e si determini, al variare di $n$,\\ la somma dei quadrati di tali radici.

\textbf{J7}$(\star\star)$. Si dimostri che per qualunque numero reale $c$ esiste una funzione biunivoca $f:\mathbb{N}^+\to\mathbb{N}^+$ tale che 
$$ \sum_{n\geq 1}\frac{\sin(f(n))}{\sqrt{f(n)}} = c.$$

\textbf{J8}$(\star\star)$. La successione $\{a_n\}_{n\geq 0}$ è definita tramite $a_n=\int_{0}^{\pi/2}\left(\sin\theta\right)^n\,d\theta$.\\ Si dimostri che è decrescente a zero e che soddisfa $a_{n+1}^2 < a_n a_{n+2}$ per ogni $n\in\mathbb{N}$.

\textbf{J9}$(\star\star)$. Si dimostri che 
$$ f(x) = \int_{2x}^{3x}\frac{t\,dt}{\arctan t} $$
definisce una funzione positiva, crescente e convessa su $\mathbb{R^+}$. \\

\textbf{J10}$(\star\star)$. Data $f:\mathbb{R}\to\mathbb{R}$ definita da
$$ f(x) = \left\{\begin{array}{ccl}0 & \text{se} &x=0\\\frac{x^2}{\arctan x}&\text{se}& x\neq 0\end{array}\right.$$
si provi che essa è biunivoca e di classe $C^{\infty}$ (ossia derivabile con continuità infinite volte) e che lo stesso vale per la sua funzione inversa $f^{-1}(x)$. Si determini infine la derivata terza nell'origine di $f^{-1}(x)$ e il valore dell'integrale
$$ \int_{0}^{4/\pi}f^{-1}(x)\arctan^2(f^{-1}(x))\,dx.$$

\textbf{J11}$(\star)$. [Integrale di Frullani] Si dimostri che per ogni $a\in\mathbb{R}^+$ si ha, in senso di Riemann improprio,
$$ \int_{0}^{+\infty}\frac{e^{-x}-e^{-ax}}{x}\,dx = \log a.$$

\textbf{J12}$(\star\star)$. Si dimostri che la successione $\{a_n\}_{n\geq 0}$ definita da
$$ a_n = \sum_{k=0}^{n}\binom{n+k}{k}\binom{n}{k}\frac{(-1)^k}{(k+1)(k+2)(k+3)} $$
è definitivamente nulla.

\textbf{J13}$(\star\star)$. Si determini il comportamento asintotico (per $n\to +\infty$) della successione $\{a_n\}_{n\geq 0}$ definita attraverso

$$ a_n = \int_{0}^{\pi/4}\left(1+\tan\theta\right)^n\,d\theta. $$

\textbf{J14}$(\star\star\star)$. Si provi che il seguente limite esiste finito e se ne determini il valore:
$$ \lim_{x\to 1^-}\left(\sqrt{1-x}\sum_{n\geq 0}x^{n^2}\right).$$

\textbf{J15}$(\star)$. Si determini la derivata decima nell'origine della funzione $f(x)=\log^2(1-x)$.

\textbf{J16}$(\star\star)$. Si determini lo sviluppo di Taylor della funzione $f(x)=\arctan x$ centrato nel punto $1$ e si dimostri che,\\ più in generale, lo sviluppo di Taylor centrato in $x_0\in\mathbb{R}$ ha raggio di convergenza 
$$ \rho_{x_0} = \sqrt{1+x_0^2}. $$

{\label{J17}\textbf{J17}}$(\star)$ [Disuguaglianza di Huygens] Si dimostri che per ogni $\theta\in\left(0,\frac{\pi}{2}\right)$ vale
$$ 2\sin\theta + \tan\theta > 3\theta.$$

\textbf{J18}$(\star\star)$ $\{a_n\}_{n\geq 0}$ è una successione per ricorrenza definita attraverso $a_0=2$ e $a_{n+1}=\sqrt{a_n^4-2}$.\\ Si dimostri che la successione diverge positivamente ma la distanza tra $a_n$ e l'intero più vicino tende a zero.

\textbf{J19}$(\star)$. Si dimostri che tra tutti i triangoli di perimetro assegnato, quelli equilateri hanno area massima. 

\textbf{J20}$(\star\star)$. $ABCD$ è un quadrilatero convesso nel piano e i suoi lati misurano nell'ordine $4,5,6,7$. Si determini quanto può valere al massimo l'area di $ABCD$ e si dimostri che le configurazioni che massimizzano l'area sono tutte e sole quelle in cui $ABCD$ è ciclico, ossia ha vertici che giacciono su una circonferenza.

\textbf{J21}$(\star\star\star)$ (Bernstein). Si determini il valore del seguente limite: 
$$ \lim_{n\to +\infty} \frac{1}{e^n}\sum_{k=0}^{n}\frac{n^k}{k!}.$$
\textbf{J22}$(\star\star)$. Si determini se esiste o meno una funzione $f\in C^0((0,1))$ tale per cui
$$ \forall n\in\mathbb{N}^+,\quad \int_{0}^{1} x^n f(x)\,dx = \frac{1}{\sqrt{n}}. $$

\textbf{J23}$(\star\star)$. $\{a_n\}_{n\geq 0}$ è una successione per ricorrenza definita attraverso
$$ a_0=\alpha,\qquad a_{n+1} = \frac{1}{2}\left(a_n+n^2\right).$$
Si determinino gli $\alpha\in\mathbb{R}$ tali per cui $a_{69}$ è strettamente più piccolo di qualunque altro elemento della successione. 

\textbf{J24}$(\star\star\star)$. Per ogni $n\in\mathbb{N}$ definiamo $d_n$ come la derivata di ordine $4n+1$ nell'origine della funzione $f(x)=\tan x$.\\ Si dimostri che per ogni $n\in\mathbb{N}^+$ si ha $d_n \in 10\mathbb{N}+6$.

\textbf{J25}$(\star\star)$. Per ogni $n\in\mathbb{N}^+$ poniamo $p_n(x)=1-x-x^n$. Si dimostri che $p_n(x)$ ha un'unica radice reale $\xi_n\in (0,1)$\\ e si determini il comportamento asintotico di $\xi_n$ per $n\to +\infty$. 

\textbf{J26}$(\star)$ Si determini l'insieme dei $\kappa\in\mathbb{R}$ tali per cui il polinomio $4x^3-\kappa x+1$ ha tre radici reali distinte.

\textbf{J27}$(\star\star)$ Si provi che il seguente limite esiste finito e se ne determini esplicitamente il valore:
$$ \lim_{n\to +\infty} \sqrt{n}\left(\frac{4}{27}\right)^n \binom{3n}{n}. $$

\textbf{J28}$(\star\star)$ (Demidovic) Si risolva l'equazione differenziale $(x-y)y'=y^2$.

\textbf{J29}$(\star\star)$ (Una sorta di problema di Keplero) Si dimostri che esiste un unico $\alpha > 1$ tale per cui la lunghezza del grafico di $f(x)=x^{\alpha}$ sull'intervallo $[0,1]$ è esattamente pari a $\frac{3}{2}$. Si stimi poi il valore di tale $\alpha$ con un errore non superiore al centesimo.

\textbf{J30}$(\star\star)$ Si discuta la convergenza della serie $\sum_{n\geq 1}\frac{\sin(n(n+1))}{n}$. 

\textbf{J31}$(\star\star)$ (Approssimazione di Ramanujan dei poveri) Detto $L(a,b)$ il perimetro di una ellisse di semiassi $a,b > 0$,\\ si dimostri che 
$$ \frac{L(a,b)}{2\pi}\in\left[\frac{a+b}{2},\sqrt{\frac{a^2+b^2}{2}}\right]. $$

\textbf{J32}$(\star\star\star)$ (Somme di Ramanujan, stavolta per davvero) Detto $\Phi_n(x)$ l'$n$-esimo polinomio ciclotomico, 
$$\Phi_n(x) = \prod_{\substack{1\leq k\leq n\\\gcd(k,n)=1}}\left(x-\exp\left(\frac{2\pi i k}{n}\right)\right),$$ e detta $\varphi$ la funzione totient di Eulero, $\varphi(n)=\#\left\{k:1\leq k\leq n, \gcd(k,n)=1\right\}$, si determini al variare di $n\in\mathbb{N}^+$\\ il coefficiente di $x^{\varphi(n)-2}$ in $\Phi_n(x)$.

\textbf{J33}$(\star)$ Si dimostri che non tutte le funzioni impropriamente Riemann-integrabili sull'intervallo $(0,1)$ realizzano l'uguaglianza
$$ \int_{0}^{1}f(x)\,dx = \lim_{N\to +\infty}\frac{1}{N}\sum_{k=1}^{N}f\left(\frac{k}{N}\right).$$

\textbf{J34}$(\star\star\star)$ (Ramanujan once again) Si dimostri l'identità
$$ 2\pi\sum_{n\geq 0}\left(\sqrt{n+1}-\sqrt{n}\right)^3 = 3\sum_{n\geq 1}\frac{1}{n\sqrt{n}}.$$

\textbf{J35}$(\star)$ Si dimostri che per ogni $x\in\mathbb{R}$ vale

$$ \sqrt{x^2+(x-1)^2}+\sqrt{(x-7)^2+(x-8)^2}\geq 10 $$

e si determini se ci sono $x$ per cui vale l'uguaglianza.

\textbf{J36}$(\star)$ Si dimostri che il seguente limite esiste finito e se ne determini il valore:
$$ \lim_{N\to +\infty}\sqrt[N]{\sum_{k=0}^{N}\binom{N}{k}\frac{k^2}{2^k}}.$$

\textbf{J37}$(\star)$ Si dimostri che il seguente limite esiste finito e se ne determini il valore:
$$ \lim_{x\to +\infty} \left(-2^x+\sqrt{4^x+2^x+x^2}\right) $$

\textbf{J38}$(\star)$ Si determini la derivata quarta nell'origine della funzione $f(x)=\cos(1-\cos x)$.

\textbf{J39}$(\star)$ Si determini il valore del seguente integrale:
$$ \int_{0}^{1}\sin^2(\arctan(\cos(\arcsin x))))\,dx $$ 

\textbf{J40}$(\star\star)$ Jack lancia una moneta equa per infinite volte e pone $a_n=1$ se l'esito dell'$n$-esimo lancio è una testa, $a_n=-1$ se l'esito dell'$n$-esimo lancio è una croce. Si dimostri che quasi certamente (ossia con probabilità $1$) la serie $\sum_{n\geq 1}\frac{a_n}{n}$ è convergente.

\textbf{J41}$(\star)$ Si dimostri che per ogni $\lambda\in\mathbb{R}$ l'equazione $x^2-\sin(x)\sinh(x)=\lambda$ ammette infinite soluzioni reali.

\textbf{J42}$(\star)$ Si determini qual è il più grande esponente $\alpha\in\mathbb{R}$ per cui si ha
$$ \cos(x)\leq\left(1-\frac{4x^2}{\pi^2}\right)^\alpha $$
per ogni $x\in\left[0,\frac{\pi}{4}\right]$.

\textbf{J43}$(\star)$ Si determini il valore del seguente integrale:
$$ I=\int_{0}^{+\infty}\left|\sin x\right| e^{-x}\,dx $$

\textbf{J44}$(\star\star)$ Considerata la successione per ricorrenza definita da $a_0=\sqrt{2}$ e $a_{n+1}=|2a_n-3|$, di dimostri che per ogni $n\in\mathbb{N}$ si ha che $a_n$ è un numero irrazionale appartenente all'intervallo $[0,3]$ e che $\{a_n\}_{n\geq 0}$ non ammette limite.

\textbf{J45}$(\star\star)$ $f:[0,+\infty)\to\mathbb{R}$ è definita come l'unica soluzione di $f''(x)=x\cdot f(x)$ che soddisfa $f(0)=1$ e $f'(0)=0$.\\ Si dimostri che 
$$ \lim_{x\to +\infty}\frac{\log f(x)}{x\sqrt{x}} = \frac{2}{3}.$$

\textbf{J46}$(\star)$ Si dimostri che per qualunque $\alpha > 1$ l'integrale $\int_{0}^{+\infty}\sin(x^\alpha)\,dx $ è convergente nel senso di Riemann improprio.

\textbf{J47}$(\star)$ Si determini per quali $\alpha\in\mathbb{R}^+$ la seguente serie converge:
$$ \sum_{n\geq 1}\frac{\binom{2n}{n}}{n^\alpha 4^n} $$

\textbf{J48}$(\star)$ Si dimostri che per ogni $n\in\mathbb{N}$ si ha
$$ \sum_{k=0}^{n}\binom{2k}{k}\binom{2n-2k}{n-k} = 4^n $$

\textbf{J49}$(\star\star)$ La successione $\{a_n\}_{n\geq 0}$ è definita da $a_0=69$ e $a_{n+1}=\log(a_n+1)$.<br> Si provi che $\lim_{n\to +\infty} na_n$ esiste finito e lo si determini esplicitamente. 

\textbf{J50}$(\star)$ Si determini sotto quali condizioni per $\alpha,\beta\in\mathbb{R}^+$ si ha che la serie
$$ \sum_{n\geq 0}\left((n+1)^\alpha-n^\alpha\right)^\beta $$
risulta convergente.

\textbf{J51}$(\star\star)$ Si dimostri che il seguente integrale è convergente e se ne determini esplicitamente il valore:
$$ \int_{0}^{+\infty}\left(\sqrt[3]{x+1}-\sqrt[3]{x}\right)^3\,dx $$

\textbf{J52}$(\star)$ Si determini esplicitamente il valore del seguente integrale e se ne deducano approssimazioni razionali per $e$: $$\int_{0}^{1}x^4(1-x)^4 e^{-x}\,dx. $$

\textbf{J53}$(\star)$ Si determini il polinomio $p(x)\in\mathbb{R}[x]$ di minimo grado il cui grafico ha tutte le seguenti proprietà:
\begin{itemize}
\item passa dall'origine e la retta tangente nell'origine ha pendenza $2$
\item passa da $(1,3)$ ed ha in tal punto un massimo relativo
\item passa da $(3,1)$ ed ha in tal punto un minimo relativo
\item passa da $(4,4)$ e in tal punto la retta tangente ha pendenza $4$.
\end{itemize}
\textbf{J54}$(\star\star)$ Sul compatto $K=\{(x,y,z)\in\mathbb{R}^3: x,y,z\in[0,1], x+y+z\leq 1\}$ si determinino la più grande costante $\alpha$ e la più piccola costante $\beta$ per cui
$$\alpha (x^3+y^3+z^3)^2\leq  (x^2+y^2+z^2)^3 \leq \beta (x^3+y^3+z^3)^2$$
valga per ogni $(x,y,z)\in K$.

\textbf{J55}$(\star\star)$ (Rudin) Nell'ipotesi che $\sum_{n\geq 0}a_n$ sia una serie convergente a termini positivi e decrescenti, si dimostri che esiste una successione $\{b_n\}_{n\geq 0}$ positiva, crescente e illimitata tale che $\sum_{n\geq 0}a_n b_n$ è ancora convergente.

\textbf{J56}$(\star)$ Dato un triangolo acutangolo $ABC$ nel piano, si dimostri che esiste un unico punto $P$ che minimizza la quantità $PA+PB+PC$, e che tale punto verifica $\widehat{APB}=\widehat{BPC}=\widehat{CPA}$.

\textbf{J57}$(\star)$ Tra tutti i cilindri circolari retti di assegnato volume, si determinino le proporzioni di quelli che hanno superficie minima.

\textbf{J58}$(\star)$ Si determini esplicitamente la distanza nel piano cartesiano tra il punto di coordinate $(1,0)$ e la parabola di equazione $y=x^2$.

\textbf{J59}$(\star)$ Si dimostri che la seguente serie è convergente e se ne determini esplicitamente il valore:
$$\sum_{n\geq 1}\left(\frac{1}{4n-1}+\frac{1}{4n-3}-\frac{1}{2n}\right).$$

\textbf{J60}$(\star\star)$ Due barche, $A$ e $B$, sono inizialmente attraccate a una certa distanza l'una dall'altra lungo un tratto rettilineo di costa. Le due barche prendono a muoversi simultaneamente e procedono entrambe con velocità di modulo costante, ma
\begin{itemize}
\item $A$ procede di moto rettilineo lungo una semiretta perpendicolare alla costa
\item $B$ procede inseguendo $A$, ossia $\vec{v}_B$ ha a qualunque tempo $t>0$ la stessa direzione del vettore $A-B$.
\end{itemize}
Si dimostri che per $t\to +\infty$ la distanza tra le due barche converge a metà della distanza iniziale.  

\textbf{J61}$(\star)$ Sapendo che $\sum_{n\geq 1}\frac{1}{n^2}=\frac{\pi^2}{6}$ si determini esplicitamente
$$ S = \sum_{n\geq 0}\frac{1}{(2n+1)^2 (2n+3)^2 (2n+5)^2} $$

\textbf{J62}$(\star)$ Si dimostri che 
$$ \int_{0}^{\pi/2}\cos(\cos x)\,dx > \frac{6}{5}.$$

\textbf{J63}$(\star)$ Si dimostri che per qualunque polinomio $p(x)\in\mathbb{R}[x]$ di grado $\leq 3$ si ha 

$$ \int_{a}^{b} p(x)\,dx = \frac{b-a}{6}\left(p(a)+4\cdot p\left(\frac{a+b}{2}\right)+p(b)\right). $$

\textbf{J64}$(\star\star)$ $f:\mathbb{Z}\times\mathbb{Z}\to (0,1)$ è una funzione che realizza
$$ f(m,n) = \frac{1}{4}\left(f(m+1,n)+f(m-1,n)+f(m,n-1)+f(m,n+1)\right) $$
per ogni $m,n\in\mathbb{Z}$. Si dimostri che $f$ è necessariamente costante.

\textbf{J65}$(\star)$ Una successione per ricorrenza $\{a_n\}_{n\geq 0}$ è definita attraverso $a_0=1$ e 
$$ a_n = 2 a_{n-1} + 4 a_{n-2} + 8 a_{n-3}+\ldots 2^n a_{0}.$$
Si determini un'espressione esplicita per $a_n$.

\textbf{J66}$(\star\star)$ Detto $\mathcal{P}$ l'insieme dei numeri primi, si dimostri che 
$$ \sum_{p\in\mathcal{P}}\frac{1}{p^2} \leq \log\sqrt{\frac{5}{2}}.$$

\textbf{J67}$(\star)$ Posto $H_n=\sum_{k=1}^{n}\frac{1}{k}$ si determini esplicitamente
$$ \sum_{n=1}^{N} n H_n. $$

\textbf{J68}$(\star\star)$(Shafer-Fink) Si provi che per ogni $x\in\mathbb{R}^+$ vale 
$$ \frac{3x}{1+2\sqrt{1+x^2}} < \arctan x < \frac{\pi x}{1+2\sqrt{1+x^2}} $$ 
e che per ogni $x> 1$ vale
$$ \log(x) > \frac{6(x-1)}{1+x+4\sqrt{x}}. $$

\textbf{J69}$(\star\star)$ $H^{(n)}$ è una matrice simmetrica e reale di dimensione $n$ che realizza $H_{ij}^{(n)}=\frac{1}{i+j-1}$.\\ Si determini esplicitamente $\det H^{(n)}$ in funzione di $n$, eventualmente osservando che
\begin{itemize}
\item $\frac{1}{i+j-1}=\int_{0}^{1}x^{i-1}\cdot x^{j-1}\,dx = \langle x^{i-1}, x^{j-1}\rangle $
\item l'ultima riga, privata dell'ultimo elemento, è necessariamente combinazione lineare delle precedenti righe private dell'ultimo elemento
\item i coefficienti di tale combinazione lineare sono fissati dalla risoluzione di un problema di interpolazione 
\item per l'invarianza del determinante sotto mosse di Gauss, la risoluzione del precedente problema esplicita il rapporto tra $\det H^{(n)}$ e $\det H^{(n-1)}$.
\end{itemize}

Infine quattro simulazioni di pre-test, con soluzioni allegate:\\

\textbf{Simulazione di pre-test \#1}, \url{https://mathb.in/76160}, soluzioni presso \url{https://mathb.in/76167}\\
\textbf{Simulazione di pre-test \#2}, \url{https://mathb.in/76170}, soluzioni presso \url{https://mathb.in/76175}\\
\textbf{Simulazione di pre-test \#3}, \url{https://mathb.in/76189}, soluzioni presso \url{https://mathb.in/76190}\\
\textbf{Simulazione di pre-test \#4}, \url{https://mathb.in/76198}, soluzioni presso \url{https://mathb.in/76204}\\

\newpage

\subsection{Soluzioni}
Segue un elenco delle soluzioni dei precedenti esercizi. Tra parentesi sono indicati trucchi o tecniche specifiche impiegati.\\

Soluzione del \textbf{J1}($\star$): \url{https://mathb.in/75940} (OGF dei binomiali centrali normalizzati)\\
Soluzione del \textbf{J2}($\star$): \url{https://mathb.in/75941} \\
Soluzione del \textbf{J3}($\star$): \url{https://mathb.in/75942} (Snake Oil per la parte difficile)\\
Soluzione del \textbf{J4}($\star$): \url{https://mathb.in/75944} (Hermite-Hadamard)\\
Soluzione del \textbf{J5}($\star$): \url{https://mathb.in/75945} \\
Soluzione del \textbf{J6}($\star$): \url{https://mathb.in/75946} (Formule di Viète e Newton, accenno ai polinomi di Legendre)\\
Soluzione del \textbf{J7}($\star\star$): \url{https://mathb.in/75939} (Riemann-Dini, sommazione per parti) \\
Soluzione del \textbf{J8}($\star\star$): \url{https://mathb.in/75947} (Cauchy-Schwarz)\\
Soluzione del \textbf{J9}($\star\star$): \url{https://mathb.in/75951} (Lemmi sulla convessità) \\
Soluzione del \textbf{J10}($\star\star$): \url{https://mathb.in/75971} (Proprietà dello sviluppo di Maclaurin di $\tan x$)\\
Soluzione del \textbf{J11}($\star$): \url{https://mathb.in/75958} \\
Soluzione del \textbf{J12}($\star\star$): \url{https://mathb.in/75972} (Rappresentazioni integrali, polinomi di Legendre)\\
Soluzione del \textbf{J13}($\star\star$): \url{https://mathb.in/75957} \\
Soluzione del \textbf{J14}($\star\star\star$): \url{https://mathb.in/75984} (Problema del cerchio di Gauss, un lemma di Analisi complessa)\\
Soluzione del \textbf{J15}($\star$): \url{https://mathb.in/75968} \\
Soluzione del \textbf{J16}($\star\star$): \url{https://mathb.in/75978} \\
Soluzione del \textbf{J17}($\star$): \url{https://mathb.in/75970} (AM-GM)\\
Soluzione del \textbf{J18}($\star\star$): \url{https://mathb.in/75977} (Polinomi di Chebyshev del primo tipo)\\
Soluzione del \textbf{J19}($\star$): \url{https://mathb.in/75998} (Principio di dualità) \\
Soluzione del \textbf{J20}($\star\star$): \url{https://mathb.in/76021} \\
Soluzione del \textbf{J21}($\star\star\star$): \url{https://mathb.in/75996} (Formula di Taylor con resto integrale, stime di momenti)\\
Soluzione del \textbf{J22}($\star\star$): \url{https://mathb.in/76009} (Trasformata di Laplace)\\
Soluzione del \textbf{J23}($\star\star$): \url{https://mathb.in/76024} (Manipolazioni di OGF, convessità)\\
Soluzione del \textbf{J24}($\star\star\star$): \url{https://mathb.in/76001} (ED non lineari, manipolazioni di EGF) \\
Soluzione del \textbf{J25}($\star\star$): \url{https://mathb.in/76016} (Metodo di Newton, funzione di Lambert)\\
Soluzione del \textbf{J26}($\star$): \url{https://mathb.in/76011} (AM-GM, polinomi di Chebyshev del primo tipo) \\
Soluzione del \textbf{J27}($\star\star$): \url{https://mathb.in/75999} (Integrali della forma $\int_{0}^{1}x^a(1-x)^b\,dx$, stime di momenti)\\
Soluzione del \textbf{J28}($\star\star$): \url{https://mathb.in/76055} (Manipolazioni di ED non lineari) \\
Soluzione del \textbf{J29}($\star\star$): \url{https://mathb.in/76052} (Metodo di Newton, un lemma sulla convessità)\\
Soluzione del \textbf{J30}($\star\star$): \url{https://mathb.in/76038} (Sommazione per parti e trucco di Van Der Corput)\\
Soluzione del \textbf{J31}($\star\star$): \url{https://mathb.in/76037} (Disuguaglianza di Jensen in forma integrale)\\
Soluzione del \textbf{J32}($\star\star\star$): \url{https://mathb.in/76053} (Formule di Viète e Newton, formula di inversione di M\"obius)  \\
Soluzione del \textbf{J33}($\star$): \url{https://mathb.in/76058} (Integrale di Dirichlet)\\
Soluzione del \textbf{J34}($\star\star\star$): \href{https://math.stackexchange.com/questions/2442091/a-ramanujan-sum/2442175#2442175}{Si veda StackExchange} (Trasformata di Laplace e numerose manipolazioni)\\
Soluzione del \textbf{J35}($\star$): \url{https://mathb.in/76050} \\
Soluzione del \textbf{J36}($\star$): \url{https://mathb.in/76036} (Azione dell'operatore $xD$ sulle OGF)\\
Soluzione del \textbf{J37}($\star$): \url{https://mathb.in/76070}\\
Soluzione del \textbf{J38}($\star$): \url{https://mathb.in/76071}\\
Soluzione del \textbf{J39}($\star$): \url{https://mathb.in/76072}\\
Soluzione del \textbf{J40}($\star\star$): \url{https://mathb.in/76073} (Disuguaglianza di Hoeffding)\\
Soluzione del \textbf{J41}($\star$): \url{https://mathb.in/76074}\\
Soluzione del \textbf{J42}($\star$): \url{https://mathb.in/76075} (Prodotto di Weierstrass del coseno)\\
Soluzione del \textbf{J43}($\star$): \url{https://mathb.in/76076}\\
Soluzione del \textbf{J44}($\star\star$): \url{https://mathb.in/76078} (Teorema di Banach-Caccioppoli)\\
Soluzione del \textbf{J45}($\star\star$): \url{https://mathb.in/76083} (Rappresentazioni integrali, stime di momenti)\\
Soluzione del \textbf{J46}($\star$): \url{https://mathb.in/76120} (Criterio di Abel-Dirichlet)\\
Soluzione del \textbf{J47}($\star$): \url{https://mathb.in/76090} (OGF dei binomiali centrali normalizzati)\\
Soluzione del \textbf{J48}($\star$): \url{https://mathb.in/76091} (Moltiplicazione di OGF)\\
Soluzione del \textbf{J49}($\star\star$): \url{https://mathb.in/76092} (Banach-Caccioppoli, sostituzioni) \\
Soluzione del \textbf{J50}($\star$): \url{https://mathb.in/76093} \\
Soluzione del \textbf{J51}($\star\star$): \url{https://mathb.in/76088} (Sostituzioni, integrali della forma $\int_{0}^{1}x^a(1-x)^b\,dx$, serie $\sum_{n\geq 1}\frac{\sin(nx)}{n}$)\\
Soluzione del \textbf{J52}($\star$): \url{https://mathb.in/76094} (Accenno agli integrali di Beukers)\\
Soluzione del \textbf{J53}($\star$): \url{https://mathb.in/76113} \\
Soluzione del \textbf{J54}($\star\star$): \url{https://mathb.in/76121} (Disuguaglianza tra le medie, coordinate sferiche)\\
Soluzione del \textbf{J55}($\star\star$): \url{https://mathb.in/76131} \\
Soluzione del \textbf{J56}($\star\star$): \url{https://mathb.in/76141}  (Convessità, Ceva)\\
Soluzione del \textbf{J57}($\star\star$): \url{https://mathb.in/76142}  (AM-GM)\\
Soluzione del \textbf{J58}($\star\star$): \url{https://mathb.in/76143}  (Metodo di Newton)\\
Soluzione del \textbf{J59}($\star$): \url{https://mathb.in/76133} \\
Soluzione del \textbf{J60}($\star\star$): \url{https://mathb.in/76144} (Lemmi di Archimede sulla parabola)\\
Soluzione del \textbf{J61}($\star\star$): \url{https://mathb.in/76145} \\
Soluzione del \textbf{J62}($\star$): \url{https://mathb.in/76134} \\
Soluzione del \textbf{J63}($\star$): \url{https://mathb.in/76146} \\
Soluzione del \textbf{J64}($\star$): \url{https://mathb.in/76151} (Principio del massimo modulo, Hales-Jewett)\\
Soluzione del \textbf{J65}($\star$): \url{https://mathb.in/76135} \\
Soluzione del \textbf{J66}($\star\star$): \url{https://mathb.in/76132} (Prodotto di Eulero, problema di Basilea)\\
Soluzione del \textbf{J67}($\star$): \url{https://mathb.in/76147} (Sommazione per parti ripetuta)\\
Soluzione del \textbf{J68}($\star\star$): \url{https://mathb.in/76152} (Diversi trucchi trigonometrici, Maclaurin di $x\cot x$)\\
Soluzione del \textbf{J69}($\star\star$): \url{https://mathb.in/76156} (Formula di Cramer, prodotti scalari, polinomi di Legendre)\\

\newpage
\section{Appendice}
In questa parte sono presenti collegamenti a brevi schede di approfondimento, e qualche riferimento bibliografico.

\subsection{Schede di approfondimento}


Concetto di \textbf{area} - \url{https://mathb.in/76086}

Formule di \textbf{Viète} - \url{https://mathb.in/75995}

\textbf{Somme di potenze consecutive} via HSI o stars\&bars - \url{https://mathb.in/75975}

\textbf{Esponenziale} e dintorni - \url{https://mathb.in/75935}

\textbf{Limiti notevoli} - \url{https://mathb.in/75954}

\textbf{Criteri di convergenza per serie}, in breve - \url{https://mathb.in/75938}

\textbf{EGZ} e \textbf{BW} - \url{https://mathb.in/75922}

Elementi di topologia, \textbf{Compattezza} e \textbf{HC} - \url{https://mathb.in/75930}

Tips\&Tricks per l'\textbf{integrazione di funzioni razionali} - \url{https://mathb.in/75931}

Disuguaglianza di \textbf{Hermite-Hadamard} - \url{https://mathb.in/76138}

\textbf{Teorema di approssimazione di Weierstrass} - \href{https://drive.google.com/file/d/1QKCejQNSohMTlZs-MPRndxo5tJ8H48Ez/view?usp=sharing}{Note di Jack} , pagina 115

\textbf{Teoremi di punto fisso} (Banach e Banach-Caccioppoli) - \url{https://mathb.in/75936}

\textbf{Illimitatezza e impropria Riemann-integrabilità} - \url{https://mathb.in/76177}

Formula di \textbf{Taylor con resto integrale} - \href{https://www2.math.upenn.edu/~kazdan/361F15/Notes/Taylor-integral.pdf}{Appunto di Kazdan}

\textbf{Integrali delle potenze del (co)seno} - \url{https://mathb.in/75993}

\textbf{Binomiali centrali e momenti} - \url{https://mathb.in/75947}

\textbf{Prodotti di Wallis, Weierstrass ed Eulero} - \url{https://mathb.in/75959}

\textbf{Approssimazione di Stirling} - \url{https://mathb.in/75976}

\textbf{Funzione Gamma di Eulero} - \href{https://quisirisolve.com/analisi-matematica/funzioni-speciali/funzione-gamma-di-eulero/teoria-ed-esercizi-sulla-funzione-gamma-di-eulero/}{Note di Jack per quisirisolve.it}


\subsection{Riferimenti bibliografici}

Per la preparazione teorica consigliamo i testi
\begin{itemize}
 \item \emph{Primo corso di Analisi Matematica} di Emilio Acerbi e Giuseppe Buttazzo. Ha una prima parte introduttiva molto \emph{reader friendly}, è molto rigoroso nella trattazione ed è ricco di appendici estremamente interessanti.
 \item \emph{Analisi matematica, Teoria ed applicazioni} di Acquistapace, Conti e Savojni. Testo di rara eleganza, ha un'impostazione squisitamente geometrica che è un peccato perdersi. \emph{Achtung}: sconfina rapidamente nell'Analisi in più variabili e nella geometria differenziale.
\end{itemize}

Per quanto concerne gli esercizi consigliamo, oltre ai $69$ sovrastanti, 
\begin{itemize}
 \item \href{https://pagine.dm.unipi.it/gobbino/Home_Page/Files/HP_AD/Eserciziari/AM1_Esercizi.pdf}{Esercizi di Analisi Matematica 1} di Marina Ghisi e Massimo Gobbino. Vasto assortimento di esercizi di medio o medio-basso livello, con qualche sconfinamento in esercizi impegnativi (riguardo successioni per ricorrenza o comportamenti qualitativi di soluzioni di equazioni differenziali, ad esempio). Un \emph{must} per farsi le ossa.
 \item \emph{Problemi scelti di Analisi Matematica 1} di Acerbi, Modica e Spagnolo. Campionario più breve del precedente, ma decisamente più impegnativo. Consigliatissimo a chi sente di avere già acquisito una solida preparazione sui fondamentali.
 
\end{itemize}



\end{document}
